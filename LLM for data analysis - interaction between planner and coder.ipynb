{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706addde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import sqlite3\n",
    "from prophet import Prophet\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "import datetime as dt\n",
    "from sklearn import linear_model\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "import openai\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from openai.api_resources.completion import Completion\n",
    "import re\n",
    "import api_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3e33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the sqlite db file and retrieve data as Pandas data frame.\n",
    "cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "sql = \"select * from fires\"\n",
    "# df = pd.read_sql_query(sql, cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "990a5d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed duplicate rows: 3525\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRE_YEAR</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>CONT_DOY</th>\n",
       "      <th>CONT_TIME</th>\n",
       "      <th>STAT_CAUSE_CODE</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>FIPS_NAME</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>combined_date_dis</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-02-02</th>\n",
       "      <td>2005</td>\n",
       "      <td>33</td>\n",
       "      <td>1300</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1730</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>0.10</td>\n",
       "      <td>40.036944</td>\n",
       "      <td>-121.005833</td>\n",
       "      <td>CA</td>\n",
       "      <td>63</td>\n",
       "      <td>Plumas</td>\n",
       "      <td>2453403.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-12</th>\n",
       "      <td>2004</td>\n",
       "      <td>133</td>\n",
       "      <td>0845</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.25</td>\n",
       "      <td>38.933056</td>\n",
       "      <td>-120.404444</td>\n",
       "      <td>CA</td>\n",
       "      <td>61</td>\n",
       "      <td>Placer</td>\n",
       "      <td>2453137.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-31</th>\n",
       "      <td>2004</td>\n",
       "      <td>152</td>\n",
       "      <td>1921</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Debris Burning</td>\n",
       "      <td>0.10</td>\n",
       "      <td>38.984167</td>\n",
       "      <td>-120.735556</td>\n",
       "      <td>CA</td>\n",
       "      <td>17</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>2453156.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-28</th>\n",
       "      <td>2004</td>\n",
       "      <td>180</td>\n",
       "      <td>1600</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.10</td>\n",
       "      <td>38.559167</td>\n",
       "      <td>-119.913333</td>\n",
       "      <td>CA</td>\n",
       "      <td>3</td>\n",
       "      <td>Alpine</td>\n",
       "      <td>2453184.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-28</th>\n",
       "      <td>2004</td>\n",
       "      <td>180</td>\n",
       "      <td>1600</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.10</td>\n",
       "      <td>38.559167</td>\n",
       "      <td>-119.933056</td>\n",
       "      <td>CA</td>\n",
       "      <td>3</td>\n",
       "      <td>Alpine</td>\n",
       "      <td>2453184.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FIRE_YEAR  DISCOVERY_DOY DISCOVERY_TIME  CONT_DOY CONT_TIME  STAT_CAUSE_CODE STAT_CAUSE_DESCR  FIRE_SIZE   LATITUDE   LONGITUDE STATE COUNTY  FIPS_NAME  DISCOVERY_DATE\n",
       "combined_date_dis                                                                                                                                                                         \n",
       "2005-02-02              2005             33           1300      33.0      1730              9.0    Miscellaneous       0.10  40.036944 -121.005833    CA     63     Plumas       2453403.5\n",
       "2004-05-12              2004            133           0845     133.0      1530              1.0        Lightning       0.25  38.933056 -120.404444    CA     61     Placer       2453137.5\n",
       "2004-05-31              2004            152           1921     152.0      2024              5.0   Debris Burning       0.10  38.984167 -120.735556    CA     17  El Dorado       2453156.5\n",
       "2004-06-28              2004            180           1600     185.0      1400              1.0        Lightning       0.10  38.559167 -119.913333    CA      3     Alpine       2453184.5\n",
       "2004-06-28              2004            180           1600     185.0      1200              1.0        Lightning       0.10  38.559167 -119.933056    CA      3     Alpine       2453184.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql_query(sql, cnx)\n",
    "df.head()\n",
    "df_interest = df[[\"FIRE_YEAR\",\"DISCOVERY_DOY\",\"DISCOVERY_TIME\",\"CONT_DOY\",\"CONT_TIME\",\"STAT_CAUSE_CODE\",\"STAT_CAUSE_DESCR\",\"FIRE_SIZE\",\"LATITUDE\",\"LONGITUDE\",\"STATE\",\"COUNTY\",\"FIPS_NAME\",\"DISCOVERY_DATE\"]]\n",
    "df_interest.isna().sum()\n",
    "print(f\"Number of removed duplicate rows: {df_interest.duplicated().sum()}\")\n",
    "df_interest = df_interest.drop_duplicates()\n",
    "df_interest[\"combined_date_dis\"] = df_interest[\"FIRE_YEAR\"]*1000 + df_interest[\"DISCOVERY_DOY\"]\n",
    "df_interest[\"combined_date_dis\"] = pd.to_datetime(df_interest[\"combined_date_dis\"], format = \"%Y%j\")\n",
    "df_interest = df_interest.set_index(\"combined_date_dis\")\n",
    "df_interest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a94884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OBJECTID', 'FOD_ID', 'FPA_ID', 'SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM', 'NWCG_REPORTING_AGENCY', 'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'SOURCE_REPORTING_UNIT', 'SOURCE_REPORTING_UNIT_NAME', 'LOCAL_FIRE_REPORT_ID', 'LOCAL_INCIDENT_ID', 'FIRE_CODE', 'FIRE_NAME', 'ICS_209_INCIDENT_NUMBER', 'ICS_209_NAME', 'MTBS_ID', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR', 'CONT_DATE', 'CONT_DOY', 'CONT_TIME', 'FIRE_SIZE', 'FIRE_SIZE_CLASS', 'LATITUDE', 'LONGITUDE', 'OWNER_CODE', 'OWNER_DESCR', 'STATE', 'COUNTY', 'FIPS_CODE', 'FIPS_NAME', 'Shape'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2637f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = 'xxxxxxxxxxxxxxxxxxx'\n",
    "\n",
    "f = open(\"columns.txt\", \"r\")\n",
    "col_txt = f.read()\n",
    "#summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06fb017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(sql_query):\n",
    "    \"\"\"\n",
    "    Executes a SQL query against the SQLite database.\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(sql_query, cnx)\n",
    "\n",
    "def generate_sql_query(messages):\n",
    "    \"\"\"\n",
    "    Uses OpenAI's GPT-4 to generate a SQL query based on a natural language prompt.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=messages\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "\n",
    "def run_tool(json_query):\n",
    "    \"\"\"\n",
    "    Uses OpenAI's GPT-4 to generate a SQL query based on a natural language prompt.\n",
    "    \"\"\"\n",
    "    if(json_query[\"tool_name\"] == \"Query_Database\"):\n",
    "        try:\n",
    "            df1 = query_database(json_query[\"command\"])\n",
    "            print(df1)\n",
    "            data = df1.head().to_dict(orient='records')  # Converts the DataFrame head to a list of dictionaries\n",
    "            return data\n",
    "        except:\n",
    "            print(\"Incorrect sql command. Trying again\")\n",
    "            return \"Retry SQL Query\"\n",
    "    if(json_query[\"tool_name\"] == \"Generate_Final_Answer\"):\n",
    "        return \"Finish\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "157bab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def llm_generate_plan(query):\n",
    "    \"\"\"\n",
    "    Generates a detailed plan using an LLM based on the provided query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): A natural language description of the task to be planned.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of actionable steps derived from the LLM's response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Specify the appropriate model\n",
    "#             temperature = 0.7,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        plan_text = response['choices'][0]['message']['content']\n",
    "        steps = plan_text.split('\\n') \n",
    "\n",
    "        return plan_text\n",
    "    except Exception as e:\n",
    "        print(\"Error in contacting the LLM:\", e)\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f60c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_numbered_list_items(text):\n",
    "    \"\"\"\n",
    "    Extracts numbered list items from the provided text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which numbered list items will be extracted.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of strings, each a numbered list item containing numbers.\n",
    "    \"\"\"\n",
    "    # Regex to find lines starting with one or more digits followed by a period and optional whitespace\n",
    "    pattern = r'\\n?(\\d+\\.\\s+.*?(?=\\n\\d+\\.|\\n\\n|\\Z))'\n",
    "    numbered_items = re.findall(pattern, text, flags=re.DOTALL)\n",
    "\n",
    "    return [item.strip() for item in numbered_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c92ed599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Plan generation\n",
    "\n",
    "def planner(query, error_str = \"None\", code = \"\", plan_txt= \"\"):\n",
    "    print(query)\n",
    "    prompt= '''<s> [INST]\n",
    "    You are a planning module specifically designed for data analysis tasks involving SQLite databases. You will receive natural language queries and must develop a clear, step-by-step plan for data analysis, directing the code generator on how to execute each step effectively. The plan should outline necessary SQL queries, data preprocessing steps, analytical methods, and final reporting formats, specifying all variable names and any new DataFrames or structures needed.\n",
    "\n",
    "    You will interact with a code generator or tool user to extract insights from a SQLite database. Ensure each instruction is precise and clear to facilitate accurate code generation. Ensure each variable is also properly defined and formatted so that the code generator makes few mistakes. Make sure you use the column names existing in the dataframe based on previous planning steps.\n",
    "\n",
    "    TASK: Draft a comprehensive, step-by-step plan to perform data analysis on the specified SQLite table named 'fires'. This table contains extensive data on wildfires. The plan should outline the necessary SQL queries, data preprocessing steps, analytical methods, and final reporting formats. Mention all variable names clearly and specify any new DataFrames or structures needed. Do not assume functions and keep changing variable names.\n",
    "    Be very clear of any variable names you provide. Do not be vague with respect to planning. Use the column names provided in the prompt.\n",
    "    DATABASE DETAILS:\n",
    "    - Database: SQLite\n",
    "    - Table Name: fires\n",
    "\n",
    "    MEMORY:\n",
    "    Context: The 'fires' table contains a comprehensive spatial database of wildfires that occurred in the United States from 1992 to 2015. This data supports national Fire Program Analysis (FPA) systems. The records, sourced from federal, state, and local fire organizations, include essential elements such as discovery date, final fire size, and point locations precise to at least the Public Land Survey System (PLSS) section (1-square mile grid). The data conforms to standards set by the National Wildfire Coordinating Group (NWCG) and includes error-checking and redundancy removal. This data publication, known as the Fire Program Analysis fire-occurrence database (FPA FOD), comprises 1.88 million geo-referenced wildfire records, representing 140 million acres burned over a 24-year period.\n",
    "\n",
    "    Column Information:\n",
    "    '''\n",
    "    prompt+= col_txt\n",
    "    prompt+='''\n",
    "    TOOLS\n",
    "    PCA_Analysis: This involves reducing the dimensionality of the dataset to identify the principal components that explain the most variance. The process will require standardizing the data, applying the PCA transformation, and interpreting the results to understand the data's underlying structure.\n",
    "    Prophet_module: Use this module to perform time series forecasting. Prepare the dataset by aggregating values into a daily sum if necessary, then rename the columns to 'ds' for the timestamp and 'y' for the variable of interest. Fit the Prophet model to the data to predict future trends and identify change points.\n",
    "    add_changepoints_to_plot: After fitting the Prophet model, use this function to visualize the trend and any detected change points in the time series, providing insights into the timing and magnitude of changes in the trend.\n",
    "    Remove_duplicates: Inspect the dataset for duplicate entries based on all or a subset of columns and remove any found duplicates to maintain data integrity.\n",
    "    LinearRegression: Instantiate and use the LinearRegression class for predictive modeling on numerical data. Ensure the data is cleaned, relevant features are selected, and any categorical variables are encoded before training the model.\n",
    "    Datetime Conversion: Convert epoch timestamps into a human-readable date format (mm/dd/yyyy) and ensure that all date-time columns are in this format for consistency and analysis.\n",
    "    Metric Extraction: Define which metrics are needed for analysis and extract them from the dataset, creating a new dataframe that includes these metrics explicitly for further analysis.\n",
    "    Classifier - XGBClassifier: Deploy the XGBClassifier for classification tasks. Preprocess the dataset to convert all feature columns into numerical data types to avoid type conversion errors during model training.\n",
    "    Label Encoder: Use this encoder to transform categorical data into a machine-readable form. Apply the encoding before model training and use the inverse transform to decode predictions back into the original categorical format.\n",
    "    RandomForestClassifier (rf_model): Implement the RandomForestClassifier with a designated variable name (rf_model). Train the model on the preprocessed dataset, ensuring all input features are in a suitable format for the classifier.\n",
    "    Classification_Report: Generate a classification report to evaluate the performance of classification models. The report should include metrics such as accuracy, precision, recall, F1 score, and support.\n",
    "    Inverse transform: After making predictions with encoded labels, use the inverse transformation to convert the predicted labels back to their original categorical values.\n",
    "    Data_Encoding: Apply data encoding methods to categorical columns using label encoders. Ensure the process includes mapping strategies for both forward transformation (encoding) and inverse transformation (decoding).\n",
    "    Split Date Format: Parse date-time columns to extract temporal features like week number, month, and day. This aids in converting date-time data into individual integer components for models that require numerical inputs.\n",
    "    Null Value Replacement: Identify columns with null or missing values and decide on a strategy to handle these, such as imputing a suitable data type and value or removing rows with missing data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Remember, your goal is to provide clear, executable instructions that a code generator can follow to generate code. Be very clear and define variables whenever needed. Use in-context example as a sample.\n",
    "\n",
    "    In-context examples:\n",
    "\n",
    "    User: 'Given the size, location and date, Predict the cause of a fire wildfire.'\n",
    "    Assistant:\n",
    "    1. Data Preparation:\n",
    "    Load and Clean Data: Import the dataset and select relevant columns such as 'FIRE_YEAR', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'DISCOVERY_DATE', 'CONT_DOY', 'CONT_TIME', 'STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'STATE', and 'FIPS_NAME'. Remove duplicate records to ensure data quality.\n",
    "    2. Feature Engineering:\n",
    "    Date and Time Conversion: Convert 'FIRE_YEAR' and 'DISCOVERY_DOY' to a datetime format to create a 'combined_date_dis' column. Similarly, combine 'FIRE_YEAR' and 'CONT_DOY' to create 'combined_date_con', ensuring both discovery and containment times are included.\n",
    "    Calculate Duration: Compute the duration of the fire by subtracting the discovery datetime from the containment datetime. Extract additional temporal features like weekday and month from the discovery date.\n",
    "    3. Exploratory Data Analysis (EDA):\n",
    "    Check Data Balance: Assess the balance of the categories within 'STAT_CAUSE_DESCR' and plan for potential data resampling if significant imbalances are detected.\n",
    "    Correlation Analysis: Calculate and visualize the correlation matrix for all numerical features to identify any strong correlations or redundancies. Drop highly correlated or redundant features to simplify the model.\n",
    "    4. Feature Engineering Continued:\n",
    "    Handle Categorical Data: Apply label encoding to categorical variables such as 'STATE' and 'FIPS_NAME' due to their high cardinality, which makes one-hot encoding impractical.\n",
    "    Feature Selection: Select significant features for the model, focusing on those most relevant for predicting fire causes, such as location, size, and date-derived features.\n",
    "    Instead of using STAT_CAUSE_CODE as label, implement LE for STAT_CAUSE_DESCR because STAT_CAUSE_CODE starts from 1.\n",
    "    Model Preparation:\n",
    "    5. Data Splitting: Divide the data into training and testing sets to ensure a robust evaluation of the model's predictive performance.\n",
    "    Handle Missing Data: Decide on strategies for handling missing data, whether imputing values or dropping rows, especially for key features used in the models.\n",
    "    Model Training and Evaluation:\n",
    "    6. Choose Algorithm: Train several machine learning model XGBoost, which are well-suited for handling the categorical nature and complexity of the dataset.\n",
    "    7. Feature Importance: Analyze the importance of different features in the model to gain insights into what factors most influence the prediction of fire causes.\n",
    "    Model Insights: Provide comprehensive insights and recommendations based on the model outcomes, highlighting key predictive factors and potential areas for further research or operational integration.\n",
    "\n",
    "\n",
    "    User: Have wildfires become more or less frequent over time?\n",
    "    Assistant:\n",
    "    1. Data Preparation\n",
    "    Load the dataset into a DataFrame named df_interest by directly querying Select relevant columns related to fire occurrences and timing, such as 'FIRE_YEAR', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'CONT_DOY', 'CONT_TIME', 'STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'STATE', 'COUNTY', 'FIPS_NAME', and 'DISCOVERY_DATE'. Store these in df_interest.\n",
    "    Check for and summarize any missing values in df_interest using the isna().sum() method.\n",
    "    Remove duplicate rows to clean the data, noting how many duplicates were removed with a print statement.\n",
    "    2. Date Conversion\n",
    "    Create a new column 'combined_date_dis' in df_interest that combines these columns into a full date format using pandas' to_datetime function, considering 'FIRE_YEAR' as the year and 'DISCOVERY_DOY' as the day of the year.\n",
    "    3. Data Aggregation\n",
    "    Set the new datetime column as the index of df_interest.\n",
    "    Aggregate data to analyze fire counts on a monthly and yearly basis using groupby and count methods. Store these aggregates in monthly_trend and yearly_trend, respectively, and then plot these trends.\n",
    "    4. Trend Analysis Using Prophet\n",
    "    Prepare the data for Prophet by resetting the index and renaming columns to 'ds' (date) and 'y' (metric, e.g., count of fires). This is necessary for compatibility with the Prophet package.\n",
    "    Define a function prophet_model that sets up and runs a Prophet model. The function should accept parameters like growth assumptions, seasonality, and the number of changepoints, and output both the model and its forecasts.\n",
    "    Apply the prophet_model function to the daily aggregated fire count data.\n",
    "    5. Change Point Analysis\n",
    "    Use Prophet's output to visualize components and trend change points. Plot the overall forecast and highlight changepoints.\n",
    "    6. Linear Regression to find change point\n",
    "    Divide the dataset into portions lower and upper of the change point using trend = forecast[[\"ds\",\"trend\"]] and using changepoints.to_list() module.\n",
    "    Analyze trends before and after the first changepoint using linear regression to determine changes in the slope of the trend lines. Convert datetime to ordinal using the required libraries.\n",
    "    6. Output Results\n",
    "    Calculate and print the slopes of the trend lines before and after the changepoint to discuss whether wildfires have become more or less frequent over time.\n",
    "\n",
    "    User: What counties are the most and least fire-prone? \\n\n",
    "    Assistant: \n",
    "    1. Execute an SQL query to extract records from the fire incidents database. Retrieve all relevant columns for state, county, and monthly fire counts for each county. Store the results in a DataFrame named monthly_fire_counts.\n",
    "    2. In the monthly_fire_counts DataFrame, merge the state and county names into a single column named 'State_County'. This facilitates easier aggregation and analysis.\n",
    "    3. Using SQL, aggregate the total number of fires, the cumulative fire duration, and the total area affected by fires for each county. Store these aggregated metrics in a new DataFrame named county_fire_stats.\n",
    "    4. Standardize the metrics (total fires, duration, and area) in the county_fire_stats DataFrame using z-scores to allow comparison across different scales and distributions.\n",
    "    5. Apply PCA to the standardized metrics in county_fire_stats to reduce dimensionality and identify the principal components that explain the most variance in the data.\n",
    "    6. Analyze the explained variance of each principal component to identify the most significant component that correlates with fire proneness.\n",
    "    7. Determine the counties with the highest and lowest scores on the principal component that shows the highest variance. These scores will indicate the most and least fire-prone counties.\n",
    "    8. Print the names of the counties that are identified as the most and least fire-prone based on the analysis.\n",
    "\n",
    "\n",
    "    [/INST]\n",
    "\n",
    "    User:\n",
    "    '''\n",
    "    prompt+= query\n",
    "    if(error_str!=\"None\"):\n",
    "        prompt = f\"\"\"\n",
    "                ### Task: Correct and Enhance Data Analysis Plan\n",
    "                ## Context:\n",
    "                - A previous attempt at generating a data analysis plan resulted in errors. The task involves correcting these errors and enhancing the plan for effective data analysis using the SQLite 'fires' table.\n",
    "\n",
    "                ## Database Details:\n",
    "                - Database: SQLite\n",
    "                - Table Name: fires\n",
    "                - Columns: {col_txt}\n",
    "\n",
    "                ## Error Details:\n",
    "                - Error Message in step no.: {error_str}\n",
    "                - Previous Code That Caused the Error:\n",
    "                ```python\n",
    "                {code}\n",
    "                ```\n",
    "\n",
    "                ## Previous Plan Steps:\n",
    "                {plan_txt}\n",
    "                \n",
    "                ### Query asked\n",
    "                {query}\n",
    "\n",
    "                ## Instructions:\n",
    "                <INST>\n",
    "                - Analyze the error and the previous code to understand what went wrong.\n",
    "                - Revise the plan steps to correct the errors identified. Ensure the new plan is robust, error-free, and tailored to the needs of the 'fires' data analysis.\n",
    "                - Clearly number and articulate each step of the new plan, starting from data preparation to final reporting. Include specific SQL queries, data preprocessing steps, analytical methods, and any necessary data formatting or transformation.\n",
    "                - Maintain consistent use of variable names and clear references to database columns as specified in the project guidelines.\n",
    "                - Ensure the plan is executable, with clear instructions for each stage of the data analysis.\n",
    "                - Make sure the error does not occur again. \n",
    "                <\\INST>\n",
    "                ### Revised Plan Steps Required:\n",
    "                - Please generate a revised set of plan steps after updating the previous plan steps, starting from step 1, correcting previous errors, and enhancing the overall data analysis approach.\n",
    "                \n",
    "                \"\"\"\n",
    "        plan_txt = llm_generate_plan(prompt)\n",
    "#             print(plan_txt)\n",
    "        return extract_numbered_list_items(plan_txt), plan_txt         \n",
    "    plan_txt = llm_generate_plan(prompt)\n",
    "    return extract_numbered_list_items(plan_txt), plan_txt\n",
    "\n",
    "\n",
    "\n",
    "#     - Print Final Answer: This is not a function. Treat this a plan termination. Just print the results. Use if answer to User's question can be given with MEMORY and data we have. Use this after thorough data analysis\n",
    "#     - Query_Database: Write only one SQL Query to query the Database. This can only execute one query at a time\n",
    "#     - PCA_Analysis: Find variance of the principal components\n",
    "#     - Prophet_module: Implement prophet module to find the trends and trend change points. (from prophet import Prophet)  Convert the inputs into daily sum and change the column names to apply fpprophet.  Convert the inputs into daily sum and change the column names to apply fpprophet. Extract yearly trends.\n",
    "#     - from prophet.plot import add_changepoints_to_plot #to find the trends in an automated fashion\n",
    "#     - Remove_duplicates: Remove duplicate rows\n",
    "#     - LinearRegression - LinearRegression()\n",
    "#     - convert datetime to required format - from 223453464 to mmddyyyy\n",
    "#     - extract the necessary metrics and store in dataframe\n",
    "#     - Classifier - XGBClassifier (before mentioning the classifier, convert all the columns into the correct datatype to avoid errors like- Error during execution: could not convert string to float: '01/01/1970' )\n",
    "#     - Label Encoder - encode when needed and decode when predicting using inverse transform or decoder\n",
    "#     - RandomForestClassifier - use variable name - rf_model\n",
    "#     - Classification_Report: Generate reports on classifier performance.\n",
    "#     - Inverse transform\n",
    "#     - Data_Encoding: Use Label Encoders for categorical data and handle inverse transformations.\n",
    "#     - Split date format into week, month, day - for integer processing\n",
    "#     - replace null type attributes with a suitable data type and value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0f8c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_code_with_reflection(error_message, previous_code, previous_plan_steps, column_info):\n",
    "    \"\"\"\n",
    "    Regenerates the plan steps for data analysis after correcting the errors in the previous attempts.\n",
    "\n",
    "    Args:\n",
    "        error_message (str): Description of the error encountered in the previous execution.\n",
    "        previous_code (str): The code that led to the error.\n",
    "        previous_plan_steps (str): The initial plan steps before the error was encountered.\n",
    "        column_info (str): Detailed information about the columns in the 'fires' database table.\n",
    "\n",
    "    Returns:\n",
    "        str: A revised set of plan steps addressing the previous errors and correctly setting up the data analysis pipeline.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    ### Task: Correct and Enhance Data Analysis Plan\n",
    "    ## Context:\n",
    "    - A previous attempt at generating a data analysis plan resulted in errors. The task involves correcting these errors and enhancing the plan for effective data analysis using the SQLite 'fires' table.\n",
    "    \n",
    "    ## Database Details:\n",
    "    - Database: SQLite\n",
    "    - Table Name: fires\n",
    "    - Columns: {column_info}\n",
    "\n",
    "    ## Error Details:\n",
    "    - Error Message: {error_message}\n",
    "    - Previous Code That Caused the Error:\n",
    "    ```python\n",
    "    {previous_code}\n",
    "    ```\n",
    "\n",
    "    ## Previous Plan Steps:\n",
    "    {previous_plan_steps}\n",
    "\n",
    "    ## Instructions:\n",
    "    - Analyze the error and the previous code to understand what went wrong.\n",
    "    - Revise the plan steps to correct the errors identified. Ensure the new plan is robust, error-free, and tailored to the needs of the 'fires' data analysis.\n",
    "    - Clearly number and articulate each step of the new plan, starting from data preparation to final reporting. Include specific SQL queries, data preprocessing steps, analytical methods, and any necessary data formatting or transformation.\n",
    "    - Maintain consistent use of variable names and clear references to database columns as specified in the project guidelines.\n",
    "    - Ensure the plan is executable, with clear instructions for each stage of the data analysis.\n",
    "\n",
    "    ### Revised Plan Steps Required:\n",
    "    - Please generate a revised set of plan steps, starting from step 1, correcting previous errors, and enhancing the overall data analysis approach.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Simulate call to an AI service to generate the corrected plan based on the prompt\n",
    "    revised_plan_steps = ai_generate_code(prompt)\n",
    "    return revised_plan_steps\n",
    "\n",
    "# # Example usage of the function:\n",
    "# column_details = \"FIRE_YEAR, DISCOVERY_DOY, DISCOVERY_TIME, CONT_DOY, etc.\"\n",
    "# error = \"NameError: name 'dataframe' is not defined\"\n",
    "# code = \"print(dataframe.head())\"\n",
    "# plan_steps = \"\"\"\n",
    "# 1. Load data into 'df_fires'.\n",
    "# 2. Summarize missing values.\n",
    "# 3. Convert date columns.\n",
    "# 4. Aggregate data for analysis.\n",
    "# \"\"\"\n",
    "# corrected_plan = regenerate_code_with_reflection(error, code, plan_steps, column_details)\n",
    "# print(corrected_plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51b9a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import yaml\n",
    "local_environment = {'df': df_interest}\n",
    "\n",
    "def load_plugin_schema(plugin_name):\n",
    "    \"\"\"\n",
    "    Loads the plugin schema from a YAML file.\n",
    "    \n",
    "    Args:\n",
    "        plugin_name (str): The name of the plugin.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The plugin schema including name, arguments, and metadata.\n",
    "    \"\"\"\n",
    "    with open(f\"{plugin_name}.yaml\", 'r') as file:\n",
    "        plugin_schema = yaml.safe_load(file)\n",
    "    return plugin_schema\n",
    "\n",
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extract Python code blocks from the provided text string.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string containing mixed content including Python code blocks.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list containing the extracted Python code blocks.\n",
    "    \"\"\"\n",
    "    # Split the text by triple backticks to find potential code blocks\n",
    "    parts = text.split('```python')\n",
    "    code_blocks = []\n",
    "\n",
    "    # Iterate over parts that come after `python` indicated by backticks\n",
    "    for part in parts[1:]:  # skip the first split as it won't contain code\n",
    "        # Find the first occurrence of triple backticks which marks the end of the code block\n",
    "        end_idx = part.find('```')\n",
    "        if end_idx != -1:\n",
    "            # Extract the code block up to the ending backticks\n",
    "            code_blocks.append(part[:end_idx].strip())\n",
    "\n",
    "    return code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "862edf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_code_with_reflection(original_code, error_message,code_prompt):\n",
    "    \"\"\"\n",
    "    Use LLM to regenerate the code based on the error encountered.\n",
    "    \"\"\"\n",
    "    prompt = f\"Correct this Python code which caused an error:\\nError: {error_message}\\nCode: {original_code}\\n# Use {code_prompt} for reference to correct it\"\n",
    "    \n",
    "    prompt = (\n",
    "        f\"### Error Correction Task\\n\"\n",
    "        f\"## Context:\\n\"\n",
    "        f\"- An error occurred during the execution of this Python code, which is part of a larger project involving data analysis.\\n\"\n",
    "        f\"- Below is the erroneous code along with the error message and the code context from the broader project.\\n\"\n",
    "        f\"- Your task is to correct the code to resolve the error and ensure it aligns with the project's objectives and coding standards.\\n\\n\"\n",
    "        f\"## Error Details:\\n\"\n",
    "        f\"- Error Message: {error_message}\\n\\n\"\n",
    "        f\"## Original Code Causing the Error:\\n\"\n",
    "        f\"```python\\n\"\n",
    "        f\"{original_code}\\n\"\n",
    "        f\"```\\n\\n\"\n",
    "        f\"## Project Code Context (Reference):\\n\"\n",
    "        f\"```python\\n\"\n",
    "        f\"{code_prompt}\\n\"\n",
    "        f\"```\\n\\n\"\n",
    "        f\"## Instructions:\\n\"\n",
    "        f\"- Correct the original code to eliminate the error.\\n\"\n",
    "        f\"- Ensure the corrected code integrates seamlessly with the provided project code context.\\n\"\n",
    "        f\"- Maintain consistency with the coding practices and standards used in the project.\\n\"\n",
    "        f\"- Provide explanations for the changes made to help understand the correction process.\\n\"\n",
    "        f\"- Make sure the type matches correctly for the classifiers\"\n",
    "        f\"- Import the correct/required libraries\"\n",
    "        f\"- Where possible, convert data types explicitly to compatible types before operation. For example, convert timedelta to numeric types (such as total seconds) if the operation requires numeric computation.\"\n",
    "        f\"- For variables like date, create new columns to support multiple datatypes like datetimelike, int, and so on\"\n",
    "        f\"- For variable type errors, try to clean the data before predictive modelling\"\n",
    "        f\"- Check the existence of the columns before dropping them\"\n",
    "        f\"- Specify datetime formats clearly - for example format='%m%d%Y for better processing\"\n",
    "        f\"- For column type errors, recheck previous code to properly correct new code\"\n",
    "        f\"<INST> Generate only python code <\\INST>\"\n",
    "    )\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\":prompt},\n",
    "        ]\n",
    "    )\n",
    "    corrected_code = response['choices'][0]['message']['content']\n",
    "#     print(\"Corrected_code\")\n",
    "#     print(corrected_code)\n",
    "    return corrected_code\n",
    "def run_code_with_custom_globals(generated_code):\n",
    "    \"\"\"\n",
    "    Executes the provided Python code using custom globals and captures the output.\n",
    "    \n",
    "    Args:\n",
    "        generated_code (str): Python code to be executed.\n",
    "        local_environment (dict): Custom global variables for the execution environment.\n",
    "    \n",
    "    Returns:\n",
    "        str: Output from executing the code.\n",
    "    \"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    redirected_output = sys.stdout = io.StringIO()   \n",
    "\n",
    "\n",
    "    try:\n",
    "        exec(generated_code, local_environment)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\", file=sys.stdout)\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "    output = redirected_output.getvalue()\n",
    "    redirected_output.close()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f32d5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_run_code(plan_steps, plugin_schemas):\n",
    "#     print(\"entered\")\n",
    "#     print(plan_steps)\n",
    "    \"\"\"\n",
    "    Generates Python code based on the given plan using specified plugins. Follow the plan as specified. Do not use additional models. Do as specified. Do not assume anything. If you need any variable, process it and used. Define variables concretely.\n",
    "    \n",
    "    Args:\n",
    "        plan_steps (list): Steps defined by the planner.\n",
    "        plugin_schemas (dict): Dictionary of loaded plugin schemas.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated Python code.\n",
    "    \"\"\"\n",
    "    code = \"\"\n",
    "    code_prompt =  '''<s> [INST]\n",
    "    Generate Python code for the given request. Import the necessary libraries first. Ensure that the generated code is ready for direct execution without requiring predefined variables outside of this context. Do not use the fbprophet package; use the Prophet package instead for trend analysis. Use from prophet import Prophet\n",
    "    DATABASE DETAILS:\n",
    "    - Database: SQLite\n",
    "    - Table Name: fires stored in df\n",
    "    - cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "    - USE cnx to query the database using sqlite commands\n",
    "\n",
    "\n",
    "    MEMORY:\n",
    "    Context: The 'fires' table contains a comprehensive spatial database of wildfires that occurred in the United States from 1992 to 2015. This data supports national Fire Program Analysis (FPA) systems. The records, sourced from federal, state, and local fire organizations, include essential elements such as discovery date, final fire size, and point locations precise to at least the Public Land Survey System (PLSS) section (1-square mile grid). The data conforms to standards set by the National Wildfire Coordinating Group (NWCG) and includes error-checking and redundancy removal. This data publication, known as the Fire Program Analysis fire-occurrence database (FPA FOD), comprises 1.88 million geo-referenced wildfire records, representing 140 million acres burned over a 24-year period.\n",
    "\n",
    "    Column Information:\n",
    "\n",
    "    '''\n",
    "    code_prompt+= col_txt\n",
    "    code_prompt+='''\n",
    "    TOOLS\n",
    "    - Print Final Answer: This is not a function. Treat this a plan termination. Just print the results. Use if answer to User's question can be given with MEMORY and data we have. Use this after thorough data analysis\n",
    "    - PCA_Analysis: Find variance of the principal components\n",
    "    - Prophet_module: Implement prophet module to find the trends and trend change points. (from prophet import Prophet)  Convert the inputs into daily sum and change the column names to apply fpprophet.  Convert the inputs into daily sum and change the column names to apply fpprophet. Extract yearly trends. Use plot_components if needed.\n",
    "    - Remove_duplicates: Remove duplicate rows\n",
    "    - Convert Julian date format into Gregorian date format.\n",
    "        df_interest[\"combined_date_dis\"] = df_interest[\"FIRE_YEAR\"]*1000 + df_interest[\"DISCOVERY_DOY\"]\n",
    "        df_interest[\"combined_date_dis\"] = pd.to_datetime(df_interest[\"combined_date_dis\"], format = \"%Y%j\")\n",
    "        df_interest = df_interest.set_index(\"combined_date_dis\")\n",
    "        df_interest.head()\n",
    "    - extract the necessary metrics and store in dataframe\n",
    "    - Classifier - XGBClassifier (before mentioning the classifier, convert all the columns into the correct datatype to avoid errors like- Error during execution: could not convert string to float: '01/01/1970' )\n",
    "    - Label Encoder - encode when needed and decode when predicting using inverse transform or decoder\n",
    "    - RandomForestClassifier - use variable name - rf_model\n",
    "    - Classification_Report: Generate reports on classifier performance.\n",
    "    - Inverse transform\n",
    "    - Data_Encoding: Use Label Encoders for categorical data and handle inverse transformations.\n",
    "    - Split date format into week, month, day - for integer processing\n",
    "    - Remove None by dropna\n",
    "    - Pandas - import pandas as pd\n",
    "    - Analysis of trend change points \n",
    "        trend = forecast[[\"ds\",\"trend\"]]\n",
    "        lower_portion = trend[(trend[\"ds\"]<m.changepoints.to_list()[0])]\n",
    "        upper_portion = trend[(trend[\"ds\"]>m.changepoints.to_list()[0])]\n",
    "\n",
    "        # Calculate slopes with using linear Regression.\n",
    "        reg_lower = linear_model.LinearRegression()\n",
    "        reg_upper = linear_model.LinearRegression()\n",
    "\n",
    "        reg_lower.fit(pd.to_datetime(lower_portion['ds']).map(dt.datetime.toordinal).values.reshape(-1, 1), lower_portion['trend'].values)\n",
    "        reg_upper.fit(pd.to_datetime(upper_portion['ds']).map(dt.datetime.toordinal).values.reshape(-1, 1), upper_portion['trend'].values)\n",
    "\n",
    "        print(f\"Slope before changepoint({m.changepoints.to_list()[0]}): {reg_lower.coef_}\")\n",
    "        print(f\"Slope before changepoint({m.changepoints.to_list()[0]}): {reg_upper.coef_}\")\n",
    "        \n",
    "    CODE GENERATION SPECIFICATIONS:\n",
    "    1. Define all intermediate data structures or variables explicitly.\n",
    "    2. Ensure all necessary libraries are imported at the beginning of the script.\n",
    "    3. Write clear comments within the code to describe each operation.\n",
    "    4. For each tool mentioned, include a code snippet that initializes the tool, performs the operation, and stores or prints the result.\n",
    "    5. Make sure to handle all data preprocessing within the script, including the conversion of date formats, encoding of categorical variables, and preparation of the dataset for machine learning models.\n",
    "    6. Avoid these errors - DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:DISCOVERY_DATE: object\n",
    "    7. Use Previous steps/CODE to define the variable names properly and reuse them if needed\n",
    "    8. Avoid variable name errors by carefully using <CODE> to define the variables\n",
    "    9. Extract relevant features which will be useful for prediction and they all follow the necessary range of values consistently\n",
    "    10. replace null type attributes with a suitable data type and value\n",
    "    11. Check within the prompt for the necessary column names. Do not assume column names\n",
    "\n",
    "    [/INST]\n",
    "    Previous steps <CODE>:\n",
    "    '''\n",
    "    counter = 0\n",
    "    f = open(\"execution_results.txt\", \"w\")\n",
    "    f_error = open(\"error_results.txt\",\"w\")\n",
    "    f_code = open(\"code_txt.txt\",\"w\")\n",
    "    error_counter =0\n",
    "    for step in plan_steps:\n",
    "        counter+=1\n",
    "        print(counter)\n",
    "        if 'plugin' in step:\n",
    "            plugin_name = step.split(':')[1].strip()\n",
    "            if plugin_name in plugin_schemas:\n",
    "                schema = plugin_schemas[plugin_name]\n",
    "                code += f\"# Using plugin: {plugin_name}\\n\"\n",
    "                code += f\"{schema['function_name']}({', '.join(f'{arg}={value}' for arg, value in schema['arguments'].items())})\\n\"\n",
    "        else:\n",
    "            # Generate general Python code based on LLM suggestions\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\":code_prompt},\n",
    "                    {\"role\": \"user\", \"content\": step}\n",
    "                ]\n",
    "            )\n",
    "            generated_code = response['choices'][0]['message']['content']\n",
    "            code += f\"{generated_code}\\n\"\n",
    "            code_formatted = f\"{generated_code}\\n\"\n",
    "            tmp = extract_python_code(code_formatted)\n",
    "            try:\n",
    "                f_code.write(str(tmp[0])+\"\\n\")    \n",
    "            except:\n",
    "                return \"Error: no code generated\", code\n",
    "            try:\n",
    "                execution_result = run_code_with_custom_globals(tmp[0])\n",
    "            except:\n",
    "                print(\"error in tmp\")\n",
    "                print(tmp)\n",
    "                return \"Error: no code generated\", code\n",
    "                break\n",
    "            if('Error' in execution_result):\n",
    "                print(\"Error detected\")\n",
    "                print(execution_result)\n",
    "                error_str = \"Error during step \" + str(counter) + \"\\n Error is\" + execution_result \n",
    "                return error_str, code\n",
    "                f_error.write(str(execution_result) + \"\\n\")\n",
    "                error_counter+=1\n",
    "                print(error_counter)\n",
    "                regenerated_code = regenerate_code_with_reflection(tmp[0],execution_result,code_prompt)\n",
    "                code += f\"{regenerated_code}\\n\"\n",
    "                code_formatted = f\"{regenerated_code}\\n\"\n",
    "                tmp = extract_python_code(code_formatted)\n",
    "                execution_result = run_code_with_custom_globals(tmp[0])\n",
    "                try:\n",
    "                    execution_result = run_code_with_custom_globals(tmp[0])\n",
    "                    f_code.write(str(tmp[0])+\"\\n\")\n",
    "                    if('Error' in execution_result):\n",
    "                        print(\"error still exists\")\n",
    "                        print(execution_result)\n",
    "                        print(tmp[0])\n",
    "                        break\n",
    "                except:\n",
    "                    print(\"error in  tmp error\")\n",
    "                    print(tmp)\n",
    "                    print(regenerated_code)\n",
    "                    print(code_formatted)\n",
    "            code_prompt+=step\n",
    "            code_prompt+=code_formatted\n",
    "            f.write(str(execution_result) + \"\\n\")\n",
    "    f.close()\n",
    "    f_error.close()\n",
    "    f_code.close()\n",
    "    return \"Done\", code\n",
    "\n",
    "\n",
    "# plugin_schemas = {}\n",
    "# error, generated_code = generate_run_code(plan_steps, plugin_schemas)\n",
    "# # tmp = extract_python_code(generated_code)\n",
    "# # print(generated_code)\n",
    "# print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71569863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query = 'What counties are the most and least fire-prone?'\n",
    "# query = 'Have wildfires become more or less frequent over time?'\n",
    "query = 'Given the size, location and date, Predict the cause of a fire wildfire.'\n",
    "# query = 'What is this dataset about?'\n",
    "# query = 'What questions can I ask?'\n",
    "# query = 'What year had the highest number of wildfires?'\n",
    "# query = 'Estimate the total property damage caused by wildfires in 2015.'\n",
    "# query = 'What factors are most predictive of fire containment times?' #not working\n",
    "# query = 'Based on historical data, predict the likelihood of a major wildfire in Texas in the next year.'\n",
    "query = 'If FIRE_SIZE is 0.1, LATITUDE is 40.036944, LONGITUDE is -121.005833, DISCOVERY_DATE is 2453403.5, predict the cause of the fire '\n",
    "# plan_steps,plan_txt = planner(query)\n",
    "# print(plan_steps)\n",
    "# plugin_schemas = {}\n",
    "# error_str, generated_code = generate_run_code(plan_steps, plugin_schemas)\n",
    "# FIRE_SIZE\tLATITUDE\tLONGITUDE\tSTATE\tCOUNTY\tFIPS_NAME\tDISCOVERY_DATE\n",
    "\n",
    "#     0.10\t40.036944\t-121.005833\tCA\t63\tPlumas\t2453403.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92379991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_analysis(query):\n",
    "    plan_steps,plan_txt = planner(query)\n",
    "    print(plan_steps)\n",
    "    plugin_schemas = {}\n",
    "    error_str, generated_code = generate_run_code(plan_steps, plugin_schemas)\n",
    "    counter = 0\n",
    "    while(\"Error\" in error_str):\n",
    "        counter+=1\n",
    "        plan_steps, plan_txt = planner(query, error_str, generated_code, plan_txt)\n",
    "        print(plan_steps)\n",
    "        error_str, generated_code = generate_run_code(plan_steps, plugin_schemas)\n",
    "        if(counter==4):\n",
    "            return error_str\n",
    "    return \"Done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b50a07fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If FIRE_SIZE is 0.1, LATITUDE is 40.036944, LONGITUDE is -121.005833, DISCOVERY_DATE is 2453403.5, predict the cause of the fire \n",
      "[\"1. **Data Preparation:**\\n   - Load and clean data: Retrieve the columns 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'DISCOVERY_DATE', 'STAT_CAUSE_DESCR' from the 'fires' table.\\n   - Check for missing values and handle them appropriately.\", \"2. **Feature Engineering:**\\n   - Convert 'DISCOVERY_DATE' from epoch timestamp format (2453403.5) to a human-readable date format (mm/dd/yyyy).\\n   - Create a combined datetime column using 'DISCOVERY_DATE' and 'DISCOVERY_TIME' if available for further analysis.\", '3. **Data Filtering:**\\n   - Filter the data based on the provided values: FIRE_SIZE = 0.1, LATITUDE = 40.036944, LONGITUDE = -121.005833, DISCOVERY_DATE = converted date value.', \"4. **Model Prediction:**\\n   - Utilize a suitable classification algorithm, such as XGBClassifier, for predicting the cause of the fire based on the filtered data.\\n   - Train the model using relevant features like 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'DISCOVERY_DATE' in the input data.\\n   - Predict the cause of the fire using the trained model based on the provided input values.\", '5. **Output:**\\n   - Generate the predicted cause of the fire based on the model prediction and provide the result to the user.']\n",
      "1\n",
      "2\n",
      "Error detected\n",
      "Error during execution: cannot convert input with unit 'D'\n",
      "\n",
      "If FIRE_SIZE is 0.1, LATITUDE is 40.036944, LONGITUDE is -121.005833, DISCOVERY_DATE is 2453403.5, predict the cause of the fire \n",
      "[\"1. **Data Preparation:**\\n   - Connect to the SQLite database 'FPA_FOD_20170508.sqlite' that contains the 'fires' table.\\n   - Retrieve the columns 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'DISCOVERY_DATE', and 'STAT_CAUSE_DESCR' from the 'fires' table.\\n   - Handle missing values by removing any rows with NaN values.\", \"2. **Feature Engineering:**\\n   - Convert the 'DISCOVERY_DATE' column from epoch timestamp format to a human-readable date format (mm/dd/yyyy).\\n   - Create a combined datetime column using 'DISCOVERY_DATE' and 'DISCOVERY_TIME' if the 'DISCOVERY_TIME' column is available.\", '3. **Data Filtering:**\\n   - Filter the data based on the following values: FIRE_SIZE = 0.1, LATITUDE = 40.036944, LONGITUDE = -121.005833, DISCOVERY_DATE = converted date value.', \"4. **Model Building and Prediction:**\\n   - Utilize a suitable classification algorithm, such as XGBClassifier or RandomForestClassifier, to predict the cause of the fire based on the filtered data.\\n   - Train the model using relevant features like 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', and 'DISCOVERY_DATE'.\\n   - Predict the cause of the fire using the trained model given the values: FIRE_SIZE = 0.1, LATITUDE = 40.036944, LONGITUDE = -121.005833, DISCOVERY_DATE = converted date value.\", '5. **Output Generation:**\\n   - Generate the predicted cause of the fire based on the model prediction.\\n   - Provide the predicted cause of the fire to the user as the final output.', \"6. **Error Prevention:**\\n   - Ensure that the conversion of 'DISCOVERY_DATE' to human-readable format does not encounter errors like the previous conversion issue.\\n   - Double-check data types and formats to avoid potential errors during data processing.\", '7. **Documentation and Reporting:**\\n   - Document any assumptions made during the analysis process.\\n   - Create a report summarizing the data analysis steps, the model used, and the predicted cause of the fire for transparency and future reference.']\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "err = perform_data_analysis(query)\n",
    "while(\"Error\" in err):\n",
    "    err = perform_data_analysis(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893a9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f51308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
