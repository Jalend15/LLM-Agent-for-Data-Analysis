{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0470399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import sqlite3\n",
    "from prophet import Prophet\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "import datetime as dt\n",
    "from sklearn import linear_model\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "import openai\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from openai.api_resources.completion import Completion\n",
    "import api_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd4a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the sqlite db file and retrieve data as Pandas data frame.\n",
    "cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "sql = \"select * from fires\"\n",
    "# df = pd.read_sql_query(sql, cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6898ca0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Index(['OBJECTID', 'FOD_ID', 'FPA_ID', 'SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM', 'NWCG_REPORTING_AGENCY', 'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'SOURCE_REPORTING_UNIT', 'SOURCE_REPORTING_UNIT_NAME', 'LOCAL_FIRE_REPORT_ID', 'LOCAL_INCIDENT_ID', 'FIRE_CODE', 'FIRE_NAME', 'ICS_209_INCIDENT_NUMBER', 'ICS_209_NAME', 'MTBS_ID', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR', 'CONT_DATE', 'CONT_DOY', 'CONT_TIME', 'FIRE_SIZE', 'FIRE_SIZE_CLASS', 'LATITUDE', 'LONGITUDE', 'OWNER_CODE', 'OWNER_DESCR', 'STATE', 'COUNTY', 'FIPS_CODE', 'FIPS_NAME', 'Shape'], dtype='object')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql_query(sql, cnx)\n",
    "df.head()\n",
    "str(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2235a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIRE_YEAR                0\n",
       "DISCOVERY_DOY            0\n",
       "DISCOVERY_TIME      882638\n",
       "CONT_DOY            891531\n",
       "CONT_TIME           972173\n",
       "STAT_CAUSE_CODE          0\n",
       "STAT_CAUSE_DESCR         0\n",
       "FIRE_SIZE                0\n",
       "LATITUDE                 0\n",
       "LONGITUDE                0\n",
       "STATE                    0\n",
       "COUNTY              678148\n",
       "FIPS_NAME           678148\n",
       "DISCOVERY_DATE           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interest = df[[\"FIRE_YEAR\",\"DISCOVERY_DOY\",\"DISCOVERY_TIME\",\"CONT_DOY\",\"CONT_TIME\",\"STAT_CAUSE_CODE\",\"STAT_CAUSE_DESCR\",\"FIRE_SIZE\",\"LATITUDE\",\"LONGITUDE\",\"STATE\",\"COUNTY\",\"FIPS_NAME\",\"DISCOVERY_DATE\"]]\n",
    "df_interest.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c6bad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed duplicate rows: 3525\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of removed duplicate rows: {df_interest.duplicated().sum()}\")\n",
    "df_interest = df_interest.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e0fd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRE_YEAR</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>CONT_DOY</th>\n",
       "      <th>CONT_TIME</th>\n",
       "      <th>STAT_CAUSE_CODE</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>FIPS_NAME</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>combined_date_dis</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-02-02</th>\n",
       "      <td>2005</td>\n",
       "      <td>33</td>\n",
       "      <td>1300</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1730</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>0.10</td>\n",
       "      <td>40.036944</td>\n",
       "      <td>-121.005833</td>\n",
       "      <td>CA</td>\n",
       "      <td>63</td>\n",
       "      <td>Plumas</td>\n",
       "      <td>2453403.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-12</th>\n",
       "      <td>2004</td>\n",
       "      <td>133</td>\n",
       "      <td>0845</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.25</td>\n",
       "      <td>38.933056</td>\n",
       "      <td>-120.404444</td>\n",
       "      <td>CA</td>\n",
       "      <td>61</td>\n",
       "      <td>Placer</td>\n",
       "      <td>2453137.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-31</th>\n",
       "      <td>2004</td>\n",
       "      <td>152</td>\n",
       "      <td>1921</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Debris Burning</td>\n",
       "      <td>0.10</td>\n",
       "      <td>38.984167</td>\n",
       "      <td>-120.735556</td>\n",
       "      <td>CA</td>\n",
       "      <td>17</td>\n",
       "      <td>El Dorado</td>\n",
       "      <td>2453156.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-28</th>\n",
       "      <td>2004</td>\n",
       "      <td>180</td>\n",
       "      <td>1600</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.10</td>\n",
       "      <td>38.559167</td>\n",
       "      <td>-119.913333</td>\n",
       "      <td>CA</td>\n",
       "      <td>3</td>\n",
       "      <td>Alpine</td>\n",
       "      <td>2453184.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-28</th>\n",
       "      <td>2004</td>\n",
       "      <td>180</td>\n",
       "      <td>1600</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.10</td>\n",
       "      <td>38.559167</td>\n",
       "      <td>-119.933056</td>\n",
       "      <td>CA</td>\n",
       "      <td>3</td>\n",
       "      <td>Alpine</td>\n",
       "      <td>2453184.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FIRE_YEAR  DISCOVERY_DOY DISCOVERY_TIME  CONT_DOY CONT_TIME  STAT_CAUSE_CODE STAT_CAUSE_DESCR  FIRE_SIZE   LATITUDE   LONGITUDE STATE COUNTY  FIPS_NAME  DISCOVERY_DATE\n",
       "combined_date_dis                                                                                                                                                                         \n",
       "2005-02-02              2005             33           1300      33.0      1730              9.0    Miscellaneous       0.10  40.036944 -121.005833    CA     63     Plumas       2453403.5\n",
       "2004-05-12              2004            133           0845     133.0      1530              1.0        Lightning       0.25  38.933056 -120.404444    CA     61     Placer       2453137.5\n",
       "2004-05-31              2004            152           1921     152.0      2024              5.0   Debris Burning       0.10  38.984167 -120.735556    CA     17  El Dorado       2453156.5\n",
       "2004-06-28              2004            180           1600     185.0      1400              1.0        Lightning       0.10  38.559167 -119.913333    CA      3     Alpine       2453184.5\n",
       "2004-06-28              2004            180           1600     185.0      1200              1.0        Lightning       0.10  38.559167 -119.933056    CA      3     Alpine       2453184.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interest[\"combined_date_dis\"] = df_interest[\"FIRE_YEAR\"]*1000 + df_interest[\"DISCOVERY_DOY\"]\n",
    "df_interest[\"combined_date_dis\"] = pd.to_datetime(df_interest[\"combined_date_dis\"], format = \"%Y%j\")\n",
    "df_interest = df_interest.set_index(\"combined_date_dis\")\n",
    "df_interest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3789654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe2e5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OBJECTID', 'FOD_ID', 'FPA_ID', 'SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM', 'NWCG_REPORTING_AGENCY', 'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'SOURCE_REPORTING_UNIT', 'SOURCE_REPORTING_UNIT_NAME', 'LOCAL_FIRE_REPORT_ID', 'LOCAL_INCIDENT_ID', 'FIRE_CODE', 'FIRE_NAME', 'ICS_209_INCIDENT_NUMBER', 'ICS_209_NAME', 'MTBS_ID', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR', 'CONT_DATE', 'CONT_DOY', 'CONT_TIME', 'FIRE_SIZE', 'FIRE_SIZE_CLASS', 'LATITUDE', 'LONGITUDE', 'OWNER_CODE', 'OWNER_DESCR', 'STATE', 'COUNTY', 'FIPS_CODE', 'FIPS_NAME', 'Shape'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2725e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = 'xxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec8f045a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac10783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"columns.txt\", \"r\")\n",
    "#summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17857c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b4f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def query_database(sql_query):\n",
    "    \"\"\"\n",
    "    Executes a SQL query against the SQLite database.\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(sql_query, cnx)\n",
    "\n",
    "def generate_sql_query(messages):\n",
    "    \"\"\"\n",
    "    Uses OpenAI's GPT-4 to generate a SQL query based on a natural language prompt.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=messages\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "\n",
    "def run_tool(json_query):\n",
    "    \"\"\"\n",
    "    Uses OpenAI's GPT-4 to generate a SQL query based on a natural language prompt.\n",
    "    \"\"\"\n",
    "    if(json_query[\"tool_name\"] == \"Query_Database\"):\n",
    "        try:\n",
    "            df1 = query_database(json_query[\"command\"])\n",
    "            print(df1)\n",
    "            data = df1.head().to_dict(orient='records')  # Converts the DataFrame head to a list of dictionaries\n",
    "            return data\n",
    "        except:\n",
    "            print(\"Incorrect sql command. Trying again\")\n",
    "            return \"Retry SQL Query\"\n",
    "    if(json_query[\"tool_name\"] == \"Generate_Final_Answer\"):\n",
    "        return \"Finish\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f326379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def llm_generate_plan(query):\n",
    "    \"\"\"\n",
    "    Generates a detailed plan using an LLM based on the provided query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): A natural language description of the task to be planned.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of actionable steps derived from the LLM's response.\n",
    "    \"\"\"\n",
    "    # Send the query to the LLM\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Specify the appropriate model\n",
    "#             temperature = 0.7,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        plan_text = response['choices'][0]['message']['content']\n",
    "#         print(\"Full response from LLM:\", plan_text)\n",
    "        \n",
    "        # Process the response into a structured plan\n",
    "        # Here you'd parse the text into discrete steps; this example assumes the LLM returns well-structured text\n",
    "        steps = plan_text.split('\\n')  # Simple split by newline; adjust based on your actual LLM response structure\n",
    "        \n",
    "#         return steps\n",
    "        return plan_text\n",
    "    except Exception as e:\n",
    "        print(\"Error in contacting the LLM:\", e)\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5ecd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import yaml\n",
    "\n",
    "def load_plugin_schema(plugin_name):\n",
    "    \"\"\"\n",
    "    Loads the plugin schema from a YAML file.\n",
    "    \n",
    "    Args:\n",
    "        plugin_name (str): The name of the plugin.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The plugin schema including name, arguments, and metadata.\n",
    "    \"\"\"\n",
    "    with open(f\"{plugin_name}.yaml\", 'r') as file:\n",
    "        plugin_schema = yaml.safe_load(file)\n",
    "    return plugin_schema\n",
    "\n",
    "def generate_code(plan_steps, plugin_schemas):\n",
    "    \"\"\"\n",
    "    Generates Python code based on the given plan using specified plugins. Follow the plan as specified. Do not use additional models. Do as specified. Do not assume anything. If you need any variable, process it and used. Define variables concretely.\n",
    "    \n",
    "    Args:\n",
    "        plan_steps (list): Steps defined by the planner.\n",
    "        plugin_schemas (dict): Dictionary of loaded plugin schemas.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated Python code.\n",
    "    \"\"\"\n",
    "    code = \"\"\n",
    "    code_prompt =  '''<s> [INST]\n",
    "    Generate Python code for the given request. Import the necessary libraries first. Ensure that the generated code is ready for direct execution without requiring predefined variables outside of this context. Do not use the fbprophet package; use the Prophet package instead for trend analysis. Use from prophet import Prophet\n",
    "    DATABASE DETAILS:\n",
    "    - Database: SQLite\n",
    "    - Table Name: fires stored in df\n",
    "    - cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "    - USE cnx to query the database using sqlite commands\n",
    "\n",
    "\n",
    "    MEMORY:\n",
    "    Context: The 'fires' table contains a comprehensive spatial database of wildfires that occurred in the United States from 1992 to 2015. This data supports national Fire Program Analysis (FPA) systems. The records, sourced from federal, state, and local fire organizations, include essential elements such as discovery date, final fire size, and point locations precise to at least the Public Land Survey System (PLSS) section (1-square mile grid). The data conforms to standards set by the National Wildfire Coordinating Group (NWCG) and includes error-checking and redundancy removal. This data publication, known as the Fire Program Analysis fire-occurrence database (FPA FOD), comprises 1.88 million geo-referenced wildfire records, representing 140 million acres burned over a 24-year period.\n",
    "\n",
    "    Column Information:\n",
    "\n",
    "    '''\n",
    "    code_prompt+= col_txt\n",
    "    code_prompt+='''\n",
    "    TOOLS\n",
    "    - Generate_Final_Answer: Use if answer to User's question can be given with MEMORY and data we have. Use this after thorough data analysis\n",
    "    - Query_Database: Write only one SQL Query to query the Database. This can only execute one query at a time\n",
    "    - PCA_Analysis: Find variance of the principal components\n",
    "    - Prophet_module: Implement prophet module to find the trends and trend change points. Use \"from prophet import Prophet\"\n",
    "    - Remove_duplicates: Remove duplicate rows\n",
    "    - convert datetime to required format\n",
    "    - extract the necessary metrics and store in dataframe\n",
    "    - Classifier - XGBClassifier - use variable name - xgb_model\n",
    "    - Label Encoder - encode when needed and decode when predicting using inverse transform or decoder\n",
    "    - RandomForestClassifier - use variable name - rf_model\n",
    "    - ClassificationReport\n",
    "    - Convert Julian time\n",
    "        \n",
    "    CODE GENERATION SPECIFICATIONS:\n",
    "    1. Define all intermediate data structures or variables explicitly.\n",
    "    2. Ensure all necessary libraries are imported at the beginning of the script.\n",
    "    3. Write clear comments within the code to describe each operation.\n",
    "    4. For each tool mentioned, include a code snippet that initializes the tool, performs the operation, and stores or prints the result.\n",
    "    5. Make sure to handle all data preprocessing within the script, including the conversion of date formats, encoding of categorical variables, and preparation of the dataset for machine learning models.\n",
    "    6. Avoid these errors - DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:DISCOVERY_DATE: object\n",
    "\n",
    "    [/INST]\n",
    "    Previous steps/CODE:\n",
    "    '''\n",
    "    for step in plan_steps:\n",
    "        if 'plugin' in step:\n",
    "            plugin_name = step.split(':')[1].strip()\n",
    "            if plugin_name in plugin_schemas:\n",
    "                schema = plugin_schemas[plugin_name]\n",
    "                code += f\"# Using plugin: {plugin_name}\\n\"\n",
    "                code += f\"{schema['function_name']}({', '.join(f'{arg}={value}' for arg, value in schema['arguments'].items())})\\n\"\n",
    "        else:\n",
    "            # Generate general Python code based on LLM suggestions\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\":code_prompt},\n",
    "                    {\"role\": \"user\", \"content\": step}\n",
    "                ]\n",
    "                \n",
    "            )\n",
    "            generated_code = response['choices'][0]['message']['content']\n",
    "            code += f\"{generated_code}\\n\"\n",
    "            code_prompt+=step\n",
    "            code_prompt+=code\n",
    "    return code\n",
    "\n",
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extract Python code blocks from the provided text string.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string containing mixed content including Python code blocks.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list containing the extracted Python code blocks.\n",
    "    \"\"\"\n",
    "    # Split the text by triple backticks to find potential code blocks\n",
    "    parts = text.split('```python')\n",
    "    code_blocks = []\n",
    "\n",
    "    # Iterate over parts that come after `python` indicated by backticks\n",
    "    for part in parts[1:]:  # skip the first split as it won't contain code\n",
    "        # Find the first occurrence of triple backticks which marks the end of the code block\n",
    "        end_idx = part.find('```')\n",
    "        if end_idx != -1:\n",
    "            # Extract the code block up to the ending backticks\n",
    "            code_blocks.append(part[:end_idx].strip())\n",
    "\n",
    "    return code_blocks\n",
    "def run_code_with_custom_globals(generated_code, local_environment):\n",
    "    \"\"\"\n",
    "    Executes the provided Python code using custom globals and captures the output.\n",
    "    \n",
    "    Args:\n",
    "        generated_code (str): Python code to be executed.\n",
    "        local_environment (dict): Custom global variables for the execution environment.\n",
    "    \n",
    "    Returns:\n",
    "        str: Output from executing the code.\n",
    "    \"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    redirected_output = sys.stdout = io.StringIO()\n",
    "\n",
    "    try:\n",
    "        exec(generated_code, local_environment)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\", file=sys.stdout)\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "    output = redirected_output.getvalue()\n",
    "    redirected_output.close()\n",
    "    return output\n",
    "#    Generate Python code for the given request. Import the necessary libraries. Generate only the code so that I can directly execute. Do not use fbprophet only use prophet\n",
    "#     The database in discussion is df (pandas dataframe).Do not assume the existence of variables. Create them if required and use them. Use proper variable names as defined, do not assume variable names.\n",
    "#     Clearly define the target variable name based on previous steps.\n",
    "#         df_model[\"combined_date_dis\"] = df_model[\"FIRE_YEAR\"]*1000 + df_model[\"DISCOVERY_DOY\"]\n",
    "#         df_model[\"combined_date_dis\"] = pd.to_datetime(df_model[\"combined_date_dis\"], format = \"%Y%j\")\n",
    "#         df_model[\"combined_date_dis\"] = df_model.combined_date_dis.astype(str) + \" \" + df_model.DISCOVERY_TIME.str[:2] + \":\" + df_model.DISCOVERY_TIME.str[2:]\n",
    "#         df_model[\"combined_date_dis\"] = pd.to_datetime(df_model[\"combined_date_dis\"])\n",
    "#         df_model[\"combined_date_con\"] = df_model[\"FIRE_YEAR\"]*1000 + df_model[\"CONT_DOY\"]\n",
    "#         df_model[\"combined_date_con\"] = pd.to_datetime(df_model[\"combined_date_con\"], format = \"%Y%j\",errors=\"ignore\")\n",
    "#         df_model[\"combined_date_con\"] = df_model.combined_date_con.astype(str) + \" \" + df_model.CONT_TIME.str[:2] + \":\" + df_model.CONT_TIME.str[2:]\n",
    "#         df_model[\"combined_date_con\"] = pd.to_datetime(df_model[\"combined_date_con\"],errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6fd1b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:\n",
      "1. Data Preparation:\n",
      "Load the dataset from the 'fires' table into a DataFrame, including columns 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'CONT_DOY', 'CONT_TIME', 'FIRE_SIZE', 'STAT_CAUSE_DESCR', 'LATITUDE', 'LONGITUDE', 'STATE', and 'FIPS_NAME'. \n",
      "Remove any duplicate records to ensure data quality.\n",
      "2. Feature Engineering:\n",
      "Convert 'DISCOVERY_DATE' and 'CONT_DATE' columns into datetime format for better temporal analysis and feature extraction.\n",
      "Calculate the duration of each fire by subtracting the discovery date from the containment date to create a new column 'FIRE_DURATION'.\n",
      "Extract additional temporal features like day of the week and month from the 'DISCOVERY_DATE' column to enrich the dataset.\n",
      "3. Exploratory Data Analysis (EDA):\n",
      "Check for missing values in the dataset and decide on appropriate handling strategies such as imputation or removal.\n",
      "Perform correlation analysis among numerical features like 'FIRE_SIZE', 'LATITUDE', and 'LONGITUDE' to identify relationships that may impact containment times.\n",
      "4. Feature Selection:\n",
      "Encode categorical variable 'STAT_CAUSE_DESCR' using label encoding for further analysis and modeling.\n",
      "Select relevant features that are most likely to predict fire containment times, considering factors like fire size, location, and cause.\n",
      "5. Splitting Data:\n",
      "Split the dataset into training and testing sets to train predictive models effectively and evaluate their performance accurately.\n",
      "Handle any missing data in the selected features before model training to ensure data integrity.\n",
      "6. Model Training and Evaluation:\n",
      "Choose suitable machine learning models like Linear Regression, Random Forest, or XGBoost to predict fire containment times based on the selected features.\n",
      "Train the models on the training dataset and evaluate their performance using metrics like Mean Absolute Error or R-squared.\n",
      "Utilize feature importance techniques to identify the most predictive factors influencing fire containment times.\n",
      "7. Final Reporting:\n",
      "Generate a comprehensive report on the predictive factors of fire containment times, highlighting the significance of different features and providing actionable insights for future analysis or interventions. \n",
      "Print the final answer with the analysis results and model predictions for fire containment times.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1. Extract monthly count of fires for each county. \\n \\n2. Combine State and Counties \\n\\n3. Retrieve the top 10 counties in terms of total fire duration. \\n\\n4. Retrieve the top 10 counties in terms of total fire size. \\n\\n5. Retrieve the top 10 counties in terms of fire counts. \\n\\n6. Combine the metrics total fire duration, total fire size,fire counts into the same data frame and standardize the values \\n\\n7. Implement PCA and calculate the variances of the principal components. \\n\\n8. Retrieve the component that has the highest variance. \\n\\n9. Print the counties that are the least and the most fire-prone. \\n\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Plan generation\n",
    "prompt= '''<s> [INST]\n",
    "You are a planning module specifically designed for data analysis tasks involving SQLite databases. You will receive natural language queries and must develop a clear, step-by-step plan for data analysis, directing the code generator on how to execute each step effectively. The plan should outline necessary SQL queries, data preprocessing steps, analytical methods, and final reporting formats, specifying all variable names and any new DataFrames or structures needed.\n",
    "\n",
    "You will interact with a code generator or tool user to extract insights from a SQLite database. Ensure each instruction is precise and clear to facilitate accurate code generation. Ensure each variable is also properly defined and formatted so that the code generator makes few mistakes. Make sure you use the column names existing in the dataframe based on previous planning steps.\n",
    "\n",
    "TASK: Draft a comprehensive, step-by-step plan to perform data analysis on the specified SQLite table named 'fires'. This table contains extensive data on wildfires. The plan should outline the necessary SQL queries, data preprocessing steps, analytical methods, and final reporting formats. Mention all variable names clearly and specify any new DataFrames or structures needed. Do not assume functions and keep changing variable names.\n",
    "Be very clear of any variable names you provide. Do not be vague with respect to planning. Use the column names provided in the prompt.\n",
    "DATABASE DETAILS:\n",
    "- Database: SQLite\n",
    "- Table Name: fires\n",
    "\n",
    "MEMORY:\n",
    "Context: The 'fires' table contains a comprehensive spatial database of wildfires that occurred in the United States from 1992 to 2015. This data supports national Fire Program Analysis (FPA) systems. The records, sourced from federal, state, and local fire organizations, include essential elements such as discovery date, final fire size, and point locations precise to at least the Public Land Survey System (PLSS) section (1-square mile grid). The data conforms to standards set by the National Wildfire Coordinating Group (NWCG) and includes error-checking and redundancy removal. This data publication, known as the Fire Program Analysis fire-occurrence database (FPA FOD), comprises 1.88 million geo-referenced wildfire records, representing 140 million acres burned over a 24-year period.\n",
    "\n",
    "Column Information:\n",
    "'''\n",
    "prompt+= col_txt\n",
    "prompt+='''\n",
    "TOOLS\n",
    "- Print Final Answer: This is not a function. Treat this a plan termination. Just print the results. Use if answer to User's question can be given with MEMORY and data we have. Use this after thorough data analysis\n",
    "- Query_Database: Write only one SQL Query to query the Database. This can only execute one query at a time\n",
    "- PCA_Analysis: Find variance of the principal components\n",
    "- Prophet_module: Implement prophet module to find the trends and trend change points. (from prophet import Prophet)  Convert the inputs into daily sum and change the column names to apply fpprophet.  Convert the inputs into daily sum and change the column names to apply fpprophet. Extract yearly trends.\n",
    "- from prophet.plot import add_changepoints_to_plot #to find the trends in an automated fashion\n",
    "- Remove_duplicates: Remove duplicate rows\n",
    "- LinearRegression - LinearRegression()\n",
    "- convert datetime to required format - from 223453464 to mmddyyyy\n",
    "- extract the necessary metrics and store in dataframe\n",
    "- Classifier - XGBClassifier (before mentioning the classifier, convert all the columns into the correct datatype to avoid errors like- Error during execution: could not convert string to float: '01/01/1970' )\n",
    "- Label Encoder - encode when needed and decode when predicting using inverse transform or decoder\n",
    "- RandomForestClassifier - use variable name - rf_model\n",
    "- Classification_Report: Generate reports on classifier performance.\n",
    "- Inverse transform\n",
    "- Data_Encoding: Use Label Encoders for categorical data and handle inverse transformations.\n",
    "- Split date format into week, month, day - for integer processing\n",
    "- replace null type attributes with a suitable data type and value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Remember, your goal is to provide clear, executable instructions that a code generator can follow to generate code. Be very clear and define variables whenever needed. Use in-context example as a sample.\n",
    "\n",
    "In-context examples:\n",
    "\n",
    "User: 'Given the size, location and date, Predict the cause of a fire wildfire.'\n",
    "Assistant:\n",
    "1. Data Preparation:\n",
    "Load and Clean Data: Import the dataset and select relevant columns such as 'FIRE_YEAR', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'DISCOVERY_DATE', 'CONT_DOY', 'CONT_TIME', 'STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'STATE', and 'FIPS_NAME'. Remove duplicate records to ensure data quality.\n",
    "2. Feature Engineering:\n",
    "Date and Time Conversion: Convert 'FIRE_YEAR' and 'DISCOVERY_DOY' to a datetime format to create a 'combined_date_dis' column. Similarly, combine 'FIRE_YEAR' and 'CONT_DOY' to create 'combined_date_con', ensuring both discovery and containment times are included.\n",
    "Calculate Duration: Compute the duration of the fire by subtracting the discovery datetime from the containment datetime. Extract additional temporal features like weekday and month from the discovery date.\n",
    "3. Exploratory Data Analysis (EDA):\n",
    "Check Data Balance: Assess the balance of the categories within 'STAT_CAUSE_DESCR' and plan for potential data resampling if significant imbalances are detected.\n",
    "Correlation Analysis: Calculate and visualize the correlation matrix for all numerical features to identify any strong correlations or redundancies. Drop highly correlated or redundant features to simplify the model.\n",
    "4. Feature Engineering Continued:\n",
    "Handle Categorical Data: Apply label encoding to categorical variables such as 'STATE' and 'FIPS_NAME' due to their high cardinality, which makes one-hot encoding impractical.\n",
    "Feature Selection: Select significant features for the model, focusing on those most relevant for predicting fire causes, such as location, size, and date-derived features.\n",
    "Instead of using STAT_CAUSE_CODE as label, implement LE for STAT_CAUSE_DESCR because STAT_CAUSE_CODE starts from 1.\n",
    "Model Preparation:\n",
    "5. Data Splitting: Divide the data into training and testing sets to ensure a robust evaluation of the model's predictive performance.\n",
    "Handle Missing Data: Decide on strategies for handling missing data, whether imputing values or dropping rows, especially for key features used in the models.\n",
    "Model Training and Evaluation:\n",
    "6. Choose Algorithm: Train several machine learning model XGBoost, which are well-suited for handling the categorical nature and complexity of the dataset.\n",
    "7. Feature Importance: Analyze the importance of different features in the model to gain insights into what factors most influence the prediction of fire causes.\n",
    "Model Insights: Provide comprehensive insights and recommendations based on the model outcomes, highlighting key predictive factors and potential areas for further research or operational integration.\n",
    "\n",
    "\n",
    "User: Have wildfires become more or less frequent over time?\n",
    "Assistant:\n",
    "1. Data Preparation\n",
    "Load the dataset into a DataFrame named df_interest by directly querying Select relevant columns related to fire occurrences and timing, such as 'FIRE_YEAR', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'CONT_DOY', 'CONT_TIME', 'STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'STATE', 'COUNTY', 'FIPS_NAME', and 'DISCOVERY_DATE'. Store these in df_interest.\n",
    "Check for and summarize any missing values in df_interest using the isna().sum() method.\n",
    "Remove duplicate rows to clean the data, noting how many duplicates were removed with a print statement.\n",
    "2. Date Conversion\n",
    "Create a new column 'combined_date_dis' in df_interest that combines these columns into a full date format using pandas' to_datetime function, considering 'FIRE_YEAR' as the year and 'DISCOVERY_DOY' as the day of the year.\n",
    "3. Data Aggregation\n",
    "Set the new datetime column as the index of df_interest.\n",
    "Aggregate data to analyze fire counts on a monthly and yearly basis using groupby and count methods. Store these aggregates in monthly_trend and yearly_trend, respectively, and then plot these trends.\n",
    "4. Trend Analysis Using Prophet\n",
    "Prepare the data for Prophet by resetting the index and renaming columns to 'ds' (date) and 'y' (metric, e.g., count of fires). This is necessary for compatibility with the Prophet package.\n",
    "Define a function prophet_model that sets up and runs a Prophet model. The function should accept parameters like growth assumptions, seasonality, and the number of changepoints, and output both the model and its forecasts.\n",
    "Apply the prophet_model function to the daily aggregated fire count data.\n",
    "5. Change Point Analysis\n",
    "Use Prophet's output to visualize components and trend change points. Plot the overall forecast and highlight changepoints.\n",
    "6. Linear Regression to find change point\n",
    "Divide the dataset into portions lower and upper of the change point using trend = forecast[[\"ds\",\"trend\"]] and using changepoints.to_list() module.\n",
    "Analyze trends before and after the first changepoint using linear regression to determine changes in the slope of the trend lines. Convert datetime to ordinal using the required libraries.\n",
    "6. Output Results\n",
    "Calculate and print the slopes of the trend lines before and after the changepoint to discuss whether wildfires have become more or less frequent over time.\n",
    "\n",
    "User: What counties are the most and least fire-prone? \\n\n",
    "Assistant: \n",
    "1. Execute an SQL query to extract records from the fire incidents database. Retrieve all relevant columns for state, county, and monthly fire counts for each county. Store the results in a DataFrame named monthly_fire_counts.\n",
    "2. In the monthly_fire_counts DataFrame, merge the state and county names into a single column named 'State_County'. This facilitates easier aggregation and analysis.\n",
    "3. Using SQL, aggregate the total number of fires, the cumulative fire duration, and the total area affected by fires for each county. Store these aggregated metrics in a new DataFrame named county_fire_stats.\n",
    "4. Standardize the metrics (total fires, duration, and area) in the county_fire_stats DataFrame using z-scores to allow comparison across different scales and distributions.\n",
    "5. Apply PCA to the standardized metrics in county_fire_stats to reduce dimensionality and identify the principal components that explain the most variance in the data.\n",
    "6. Analyze the explained variance of each principal component to identify the most significant component that correlates with fire proneness.\n",
    "7. Determine the counties with the highest and lowest scores on the principal component that shows the highest variance. These scores will indicate the most and least fire-prone counties.\n",
    "8. Print the names of the counties that are identified as the most and least fire-prone based on the analysis.\n",
    "\n",
    "\n",
    "[/INST]\n",
    "\n",
    "User:\n",
    "'''\n",
    "# query = 'What counties are the most and least fire-prone?'\n",
    "# query = 'Have wildfires become more or less frequent over time?'\n",
    "# query = 'Given the size, location and date, Predict the cause of a fire wildfire.'\n",
    "# query = 'What is this dataset about?'\n",
    "# query = 'What questions can I ask?'\n",
    "# query = 'What year had the highest number of wildfires?'\n",
    "# query = 'Estimate the total property damage caused by wildfires in 2015.'\n",
    "query = 'What factors are most predictive of fire containment times?' #not working\n",
    "# query = 'Based on historical data, predict the likelihood of a major wildfire in Texas in the next year.'\n",
    "prompt+= query\n",
    "plan_txt = llm_generate_plan(prompt)\n",
    "print(plan_txt)\n",
    "# print(\"Generated Plan Steps:\")\n",
    "#TASK: Draft a comprehensive, step-by-step plan to perform data analysis on a specified table. Mention the variable names clearly and the new dataframes needed for the code generator to understand.\n",
    "\n",
    "\n",
    "# Assistant:\n",
    "# 1. Extract monthly counts of fires for each county using SQL:\n",
    "#    - `monthly_fires_df = QUERY('SELECT state, county, COUNT(*) AS monthly_fire_count, strftime('%m', discovery_date) AS month FROM fires GROUP BY state, county, month');`\n",
    "# 2. Combine state and county into a unique identifier:\n",
    "#    - `monthly_fires_df['county_identifier'] = monthly_fires_df['state'] + ', ' + monthly_fires_df['county'];`\n",
    "# 3. Retrieve and rank the top 10 counties by total fire duration, size, and counts:\n",
    "#    - `top_duration_df = QUERY('SELECT county_identifier, SUM(duration) AS total_duration FROM monthly_fires_df GROUP BY county_identifier ORDER BY total_duration DESC LIMIT 10');`\n",
    "#    - Similar queries for fire size and counts.\n",
    "# 4. Combine these metrics into a single DataFrame and standardize the values:\n",
    "#    - `combined_metrics_df = MERGE(top_duration_df, top_size_df, top_counts_df);`\n",
    "#    - `standardized_metrics_df = STANDARDIZE(combined_metrics_df);`\n",
    "# 5. Implement PCA on the standardized data and identify the principal component with the highest variance.\n",
    "#    - `pca_results = PCA_ANALYSIS(standardized_metrics_df);`\n",
    "# 6. Output the counties from the component showing the highest variance as the most and least fire-prone.\n",
    "#    - `final_result = IDENTIFY_EXTREMES(pca_results);`\n",
    "#    - `PRINT('Most and Least Fire-Prone Counties: ', final_result);`\n",
    "# Assistant: Extract monthly count of fires for each county. \\n \n",
    "# Combine State and Counties \\n\n",
    "# Retrieve the top 10 counties in terms of total fire duration. \\n\n",
    "# Retrieve the top 10 counties in terms of total fire size. \\n\n",
    "# Retrieve the top 10 counties in terms of fire counts. \\n\n",
    "# Combine the metrics total fire duration, total fire size,fire counts into the same data frame and standardize the values \\n\n",
    "# Implement PCA and calculate the variances of the principal components. \\n\n",
    "# Retrieve the component that has the highest varience. \\n\n",
    "# Print the counties that are the least and the most fire-prone. \\n\n",
    "\n",
    "'''\n",
    "1. Extract monthly count of fires for each county. \\n \n",
    "2. Combine State and Counties \\n\n",
    "3. Retrieve the top 10 counties in terms of total fire duration. \\n\n",
    "4. Retrieve the top 10 counties in terms of total fire size. \\n\n",
    "5. Retrieve the top 10 counties in terms of fire counts. \\n\n",
    "6. Combine the metrics total fire duration, total fire size,fire counts into the same data frame and standardize the values \\n\n",
    "7. Implement PCA and calculate the variances of the principal components. \\n\n",
    "8. Retrieve the component that has the highest variance. \\n\n",
    "9. Print the counties that are the least and the most fire-prone. \\n\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e558c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_numbered_list_items(text):\n",
    "    \"\"\"\n",
    "    Extracts numbered list items from the provided text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which numbered list items will be extracted.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of strings, each a numbered list item containing numbers.\n",
    "    \"\"\"\n",
    "    # Regex to find lines starting with one or more digits followed by a period and optional whitespace\n",
    "    pattern = r'\\n?(\\d+\\.\\s+.*?(?=\\n\\d+\\.|\\n\\n|\\Z))'\n",
    "    numbered_items = re.findall(pattern, text, flags=re.DOTALL)\n",
    "\n",
    "    return [item.strip() for item in numbered_items]\n",
    "plan_steps = extract_numbered_list_items(plan_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20bb5e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1. Data Preparation:\\nLoad the dataset from the 'fires' table into a DataFrame, including columns 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'CONT_DOY', 'CONT_TIME', 'FIRE_SIZE', 'STAT_CAUSE_DESCR', 'LATITUDE', 'LONGITUDE', 'STATE', and 'FIPS_NAME'. \\nRemove any duplicate records to ensure data quality.\",\n",
       " \"2. Feature Engineering:\\nConvert 'DISCOVERY_DATE' and 'CONT_DATE' columns into datetime format for better temporal analysis and feature extraction.\\nCalculate the duration of each fire by subtracting the discovery date from the containment date to create a new column 'FIRE_DURATION'.\\nExtract additional temporal features like day of the week and month from the 'DISCOVERY_DATE' column to enrich the dataset.\",\n",
       " \"3. Exploratory Data Analysis (EDA):\\nCheck for missing values in the dataset and decide on appropriate handling strategies such as imputation or removal.\\nPerform correlation analysis among numerical features like 'FIRE_SIZE', 'LATITUDE', and 'LONGITUDE' to identify relationships that may impact containment times.\",\n",
       " \"4. Feature Selection:\\nEncode categorical variable 'STAT_CAUSE_DESCR' using label encoding for further analysis and modeling.\\nSelect relevant features that are most likely to predict fire containment times, considering factors like fire size, location, and cause.\",\n",
       " '5. Splitting Data:\\nSplit the dataset into training and testing sets to train predictive models effectively and evaluate their performance accurately.\\nHandle any missing data in the selected features before model training to ensure data integrity.',\n",
       " '6. Model Training and Evaluation:\\nChoose suitable machine learning models like Linear Regression, Random Forest, or XGBoost to predict fire containment times based on the selected features.\\nTrain the models on the training dataset and evaluate their performance using metrics like Mean Absolute Error or R-squared.\\nUtilize feature importance techniques to identify the most predictive factors influencing fire containment times.',\n",
       " '7. Final Reporting:\\nGenerate a comprehensive report on the predictive factors of fire containment times, highlighting the significance of different features and providing actionable insights for future analysis or interventions. \\nPrint the final answer with the analysis results and model predictions for fire containment times.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08f490f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import yaml\n",
    "local_environment = {'df': df_interest}\n",
    "\n",
    "def load_plugin_schema(plugin_name):\n",
    "    \"\"\"\n",
    "    Loads the plugin schema from a YAML file.\n",
    "    \n",
    "    Args:\n",
    "        plugin_name (str): The name of the plugin.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The plugin schema including name, arguments, and metadata.\n",
    "    \"\"\"\n",
    "    with open(f\"{plugin_name}.yaml\", 'r') as file:\n",
    "        plugin_schema = yaml.safe_load(file)\n",
    "    return plugin_schema\n",
    "\n",
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extract Python code blocks from the provided text string.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string containing mixed content including Python code blocks.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list containing the extracted Python code blocks.\n",
    "    \"\"\"\n",
    "    # Split the text by triple backticks to find potential code blocks\n",
    "    parts = text.split('```python')\n",
    "    code_blocks = []\n",
    "\n",
    "    # Iterate over parts that come after `python` indicated by backticks\n",
    "    for part in parts[1:]:  # skip the first split as it won't contain code\n",
    "        # Find the first occurrence of triple backticks which marks the end of the code block\n",
    "        end_idx = part.find('```')\n",
    "        if end_idx != -1:\n",
    "            # Extract the code block up to the ending backticks\n",
    "            code_blocks.append(part[:end_idx].strip())\n",
    "\n",
    "    return code_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19471c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_code_with_reflection(original_code, error_message,code_prompt):\n",
    "    \"\"\"\n",
    "    Use LLM to regenerate the code based on the error encountered.\n",
    "    \"\"\"\n",
    "    prompt = f\"Correct this Python code which caused an error:\\nError: {error_message}\\nCode: {original_code}\\n# Use {code_prompt} for reference to correct it\"\n",
    "    \n",
    "    prompt = (\n",
    "        f\"### Error Correction Task\\n\"\n",
    "        f\"## Context:\\n\"\n",
    "        f\"- An error occurred during the execution of this Python code, which is part of a larger project involving data analysis.\\n\"\n",
    "        f\"- Below is the erroneous code along with the error message and the code context from the broader project.\\n\"\n",
    "        f\"- Your task is to correct the code to resolve the error and ensure it aligns with the project's objectives and coding standards.\\n\\n\"\n",
    "        f\"## Error Details:\\n\"\n",
    "        f\"- Error Message: {error_message}\\n\\n\"\n",
    "        f\"## Original Code Causing the Error:\\n\"\n",
    "        f\"```python\\n\"\n",
    "        f\"{original_code}\\n\"\n",
    "        f\"```\\n\\n\"\n",
    "        f\"## Project Code Context (Reference):\\n\"\n",
    "        f\"```python\\n\"\n",
    "        f\"{code_prompt}\\n\"\n",
    "        f\"```\\n\\n\"\n",
    "        f\"## Instructions:\\n\"\n",
    "        f\"- Correct the original code to eliminate the error.\\n\"\n",
    "        f\"- Ensure the corrected code integrates seamlessly with the provided project code context.\\n\"\n",
    "        f\"- Maintain consistency with the coding practices and standards used in the project.\\n\"\n",
    "        f\"- Provide explanations for the changes made to help understand the correction process.\\n\"\n",
    "        f\"- Make sure the type matches correctly for the classifiers\"\n",
    "        f\"- Import the correct/required libraries\"\n",
    "        f\"- Where possible, convert data types explicitly to compatible types before operation. For example, convert timedelta to numeric types (such as total seconds) if the operation requires numeric computation.\"\n",
    "        f\"- For variables like date, create new columns to support multiple datatypes like datetimelike, int, and so on\"\n",
    "        f\"- For variable type errors, try to clean the data before predictive modelling\"\n",
    "        f\"- Check the existence of the columns before dropping them\"\n",
    "        f\"- Specify datetime formats clearly - for example format='%m%d%Y for better processing\"\n",
    "        f\"- For column type errors, recheck previous code to properly correct new code\"\n",
    "        f\"<INST> Generate only python code <\\INST>\"\n",
    "    )\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\":prompt},\n",
    "        ]\n",
    "    )\n",
    "    corrected_code = response['choices'][0]['message']['content']\n",
    "#     print(\"Corrected_code\")\n",
    "#     print(corrected_code)\n",
    "    return corrected_code\n",
    "def run_code_with_custom_globals(generated_code):\n",
    "    \"\"\"\n",
    "    Executes the provided Python code using custom globals and captures the output.\n",
    "    \n",
    "    Args:\n",
    "        generated_code (str): Python code to be executed.\n",
    "        local_environment (dict): Custom global variables for the execution environment.\n",
    "    \n",
    "    Returns:\n",
    "        str: Output from executing the code.\n",
    "    \"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    redirected_output = sys.stdout = io.StringIO()   \n",
    "    tmp = \"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        exec(generated_code, local_environment)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\", file=sys.stdout)\n",
    "        \n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "    output = redirected_output.getvalue()\n",
    "    redirected_output.close()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d508a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered\n",
      "[\"1. Data Preparation:\\nLoad the dataset from the 'fires' table into a DataFrame, including columns 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'CONT_DOY', 'CONT_TIME', 'FIRE_SIZE', 'STAT_CAUSE_DESCR', 'LATITUDE', 'LONGITUDE', 'STATE', and 'FIPS_NAME'. \\nRemove any duplicate records to ensure data quality.\", \"2. Feature Engineering:\\nConvert 'DISCOVERY_DATE' and 'CONT_DATE' columns into datetime format for better temporal analysis and feature extraction.\\nCalculate the duration of each fire by subtracting the discovery date from the containment date to create a new column 'FIRE_DURATION'.\\nExtract additional temporal features like day of the week and month from the 'DISCOVERY_DATE' column to enrich the dataset.\", \"3. Exploratory Data Analysis (EDA):\\nCheck for missing values in the dataset and decide on appropriate handling strategies such as imputation or removal.\\nPerform correlation analysis among numerical features like 'FIRE_SIZE', 'LATITUDE', and 'LONGITUDE' to identify relationships that may impact containment times.\", \"4. Feature Selection:\\nEncode categorical variable 'STAT_CAUSE_DESCR' using label encoding for further analysis and modeling.\\nSelect relevant features that are most likely to predict fire containment times, considering factors like fire size, location, and cause.\", '5. Splitting Data:\\nSplit the dataset into training and testing sets to train predictive models effectively and evaluate their performance accurately.\\nHandle any missing data in the selected features before model training to ensure data integrity.', '6. Model Training and Evaluation:\\nChoose suitable machine learning models like Linear Regression, Random Forest, or XGBoost to predict fire containment times based on the selected features.\\nTrain the models on the training dataset and evaluate their performance using metrics like Mean Absolute Error or R-squared.\\nUtilize feature importance techniques to identify the most predictive factors influencing fire containment times.', '7. Final Reporting:\\nGenerate a comprehensive report on the predictive factors of fire containment times, highlighting the significance of different features and providing actionable insights for future analysis or interventions. \\nPrint the final answer with the analysis results and model predictions for fire containment times.']\n",
      "0\n",
      "1\n",
      "2\n",
      "Error detected\n",
      "Error during execution: 'CONT_DATE'\n",
      "\n",
      "1\n",
      "error still exists\n",
      "Error during execution: unsupported operand type(s) for +: 'DatetimeArray' and 'str'\n",
      "\n",
      "# Correcting the code to resolve the error related to 'CONT_DATE' column\n",
      "import pandas as pd\n",
      "\n",
      "# Creating 'DISCOVERY_DATE' and 'CONT_DATE' columns by combining date and time columns\n",
      "df['DISCOVERY_DATE'] = pd.to_datetime(df['DISCOVERY_DATE'] + ' ' + df['DISCOVERY_TIME'])\n",
      "df['CONT_DATE'] = pd.to_datetime(df['CONT_DATE'] + ' ' + df['CONT_TIME'])\n",
      "\n",
      "# Calculating the duration of each fire to create a new column 'FIRE_DURATION'\n",
      "df['FIRE_DURATION'] = df['CONT_DATE'] - df['DISCOVERY_DATE']\n",
      "\n",
      "# Extracting additional temporal features like day of the week and month\n",
      "df['DISCOVERY_DAY_OF_WEEK'] = df['DISCOVERY_DATE'].dt.dayofweek\n",
      "df['DISCOVERY_MONTH'] = df['DISCOVERY_DATE'].dt.month\n",
      "\n",
      "# Dropping the original time columns after creating the combined datetime columns\n",
      "df.drop(columns=['DISCOVERY_TIME', 'CONT_TIME'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_run_code(plan_steps, plugin_schemas):\n",
    "    print(\"entered\")\n",
    "    print(plan_steps)\n",
    "    \"\"\"\n",
    "    Generates Python code based on the given plan using specified plugins. Follow the plan as specified. Do not use additional models. Do as specified. Do not assume anything. If you need any variable, process it and used. Define variables concretely.\n",
    "    \n",
    "    Args:\n",
    "        plan_steps (list): Steps defined by the planner.\n",
    "        plugin_schemas (dict): Dictionary of loaded plugin schemas.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated Python code.\n",
    "    \"\"\"\n",
    "    code = \"\"\n",
    "    code_prompt =  '''<s> [INST]\n",
    "    Generate Python code for the given request. Import the necessary libraries first. Ensure that the generated code is ready for direct execution without requiring predefined variables outside of this context. Do not use the fbprophet package; use the Prophet package instead for trend analysis. Use from prophet import Prophet\n",
    "    DATABASE DETAILS:\n",
    "    - Database: SQLite\n",
    "    - Table Name: fires stored in df\n",
    "    - cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "    - USE cnx to query the database using sqlite commands\n",
    "\n",
    "\n",
    "    MEMORY:\n",
    "    Context: The 'fires' table contains a comprehensive spatial database of wildfires that occurred in the United States from 1992 to 2015. This data supports national Fire Program Analysis (FPA) systems. The records, sourced from federal, state, and local fire organizations, include essential elements such as discovery date, final fire size, and point locations precise to at least the Public Land Survey System (PLSS) section (1-square mile grid). The data conforms to standards set by the National Wildfire Coordinating Group (NWCG) and includes error-checking and redundancy removal. This data publication, known as the Fire Program Analysis fire-occurrence database (FPA FOD), comprises 1.88 million geo-referenced wildfire records, representing 140 million acres burned over a 24-year period.\n",
    "\n",
    "    Column Information:\n",
    "\n",
    "    '''\n",
    "    code_prompt+= col_txt\n",
    "    code_prompt+='''\n",
    "    TOOLS\n",
    "    - Print Final Answer: This is not a function. Treat this a plan termination. Just print the results. Use if answer to User's question can be given with MEMORY and data we have. Use this after thorough data analysis\n",
    "    - Query_Database: Write only one SQL Query to query the Database. This can only execute one query at a time\n",
    "    - PCA_Analysis: Find variance of the principal components\n",
    "    - Prophet_module: Implement prophet module to find the trends and trend change points. (from prophet import Prophet)  Convert the inputs into daily sum and change the column names to apply fpprophet.  Convert the inputs into daily sum and change the column names to apply fpprophet. Extract yearly trends. Use plot_components if needed.\n",
    "    - Remove_duplicates: Remove duplicate rows\n",
    "    - Convert Julian date format into Gregorian date format.\n",
    "        df_interest[\"combined_date_dis\"] = df_interest[\"FIRE_YEAR\"]*1000 + df_interest[\"DISCOVERY_DOY\"]\n",
    "        df_interest[\"combined_date_dis\"] = pd.to_datetime(df_interest[\"combined_date_dis\"], format = \"%Y%j\")\n",
    "        df_interest = df_interest.set_index(\"combined_date_dis\")\n",
    "        df_interest.head()\n",
    "    - extract the necessary metrics and store in dataframe\n",
    "    - Classifier - XGBClassifier (before mentioning the classifier, convert all the columns into the correct datatype to avoid errors like- Error during execution: could not convert string to float: '01/01/1970' )\n",
    "    - Label Encoder - encode when needed and decode when predicting using inverse transform or decoder\n",
    "    - RandomForestClassifier - use variable name - rf_model\n",
    "    - Classification_Report: Generate reports on classifier performance.\n",
    "    - Inverse transform\n",
    "    - Data_Encoding: Use Label Encoders for categorical data and handle inverse transformations.\n",
    "    - Split date format into week, month, day - for integer processing\n",
    "    - Remove None by dropna\n",
    "    - Pandas - import pandas as pd\n",
    "    - Analysis of trend change points \n",
    "        trend = forecast[[\"ds\",\"trend\"]]\n",
    "        lower_portion = trend[(trend[\"ds\"]<m.changepoints.to_list()[0])]\n",
    "        upper_portion = trend[(trend[\"ds\"]>m.changepoints.to_list()[0])]\n",
    "\n",
    "        # Calculate slopes with using linear Regression.\n",
    "        reg_lower = linear_model.LinearRegression()\n",
    "        reg_upper = linear_model.LinearRegression()\n",
    "\n",
    "        reg_lower.fit(pd.to_datetime(lower_portion['ds']).map(dt.datetime.toordinal).values.reshape(-1, 1), lower_portion['trend'].values)\n",
    "        reg_upper.fit(pd.to_datetime(upper_portion['ds']).map(dt.datetime.toordinal).values.reshape(-1, 1), upper_portion['trend'].values)\n",
    "\n",
    "        print(f\"Slope before changepoint({m.changepoints.to_list()[0]}): {reg_lower.coef_}\")\n",
    "        print(f\"Slope before changepoint({m.changepoints.to_list()[0]}): {reg_upper.coef_}\")\n",
    "        \n",
    "    CODE GENERATION SPECIFICATIONS:\n",
    "    1. Define all intermediate data structures or variables explicitly.\n",
    "    2. Ensure all necessary libraries are imported at the beginning of the script.\n",
    "    3. Write clear comments within the code to describe each operation.\n",
    "    4. For each tool mentioned, include a code snippet that initializes the tool, performs the operation, and stores or prints the result.\n",
    "    5. Make sure to handle all data preprocessing within the script, including the conversion of date formats, encoding of categorical variables, and preparation of the dataset for machine learning models.\n",
    "    6. Avoid these errors - DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:DISCOVERY_DATE: object\n",
    "    7. Use Previous steps/CODE to define the variable names properly and reuse them if needed\n",
    "    8. Avoid variable name errors by carefully using <CODE> to define the variables\n",
    "    9. Extract relevant features which will be useful for prediction and they all follow the necessary range of values consistently\n",
    "    10. replace null type attributes with a suitable data type and value\n",
    "    11. Check within the prompt for the necessary column names. Do not assume column names\n",
    "\n",
    "    [/INST]\n",
    "    Previous steps <CODE>:\n",
    "    '''\n",
    "    counter = 0\n",
    "    f = open(\"execution_results.txt\", \"w\")\n",
    "    f_error = open(\"error_results.txt\",\"w\")\n",
    "    f_code = open(\"code_txt.txt\",\"w\")\n",
    "    error_counter =0\n",
    "    print(counter)\n",
    "#     local_environment = {'df': df}\n",
    "    for step in plan_steps:\n",
    "        counter+=1\n",
    "        print(counter)\n",
    "        if 'plugin' in step:\n",
    "            plugin_name = step.split(':')[1].strip()\n",
    "            if plugin_name in plugin_schemas:\n",
    "                schema = plugin_schemas[plugin_name]\n",
    "                code += f\"# Using plugin: {plugin_name}\\n\"\n",
    "                code += f\"{schema['function_name']}({', '.join(f'{arg}={value}' for arg, value in schema['arguments'].items())})\\n\"\n",
    "        else:\n",
    "            # Generate general Python code based on LLM suggestions\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\":code_prompt},\n",
    "                    {\"role\": \"user\", \"content\": step}\n",
    "                ]\n",
    "            )\n",
    "            generated_code = response['choices'][0]['message']['content']\n",
    "            code += f\"{generated_code}\\n\"\n",
    "            code_formatted = f\"{generated_code}\\n\"\n",
    "            tmp = extract_python_code(code_formatted)\n",
    "            \n",
    "            if(len(tmp)==0):\n",
    "                print(\"jalend\")\n",
    "                print(step)\n",
    "                print(generated_code)\n",
    "                break\n",
    "            f_code.write(str(tmp[0])+\"\\n\")    \n",
    "#             print(\"Step asked\")\n",
    "#             print(step)\n",
    "#             print(\"First generated code\")\n",
    "#             print(generated_code)\n",
    "#             print(\"Code extracted first\")\n",
    "#             print(tmp)\n",
    "#             execution_result = run_code_with_custom_globals(tmp[0])\n",
    "            try:\n",
    "                execution_result = run_code_with_custom_globals(tmp[0])\n",
    "            except:\n",
    "                print(\"error in tmp\")\n",
    "                print(tmp)\n",
    "                break\n",
    "#             print(execution_result)\n",
    "            if('Error' in execution_result):\n",
    "                print(\"Error detected\")\n",
    "                print(execution_result)\n",
    "                f_error.write(str(execution_result) + \"\\n\")\n",
    "                error_counter+=1\n",
    "                print(error_counter)\n",
    "#                 print(execution_result)\n",
    "                regenerated_code = regenerate_code_with_reflection(tmp[0],execution_result,code_prompt)\n",
    "                code += f\"{regenerated_code}\\n\"\n",
    "                code_formatted = f\"{regenerated_code}\\n\"\n",
    "                tmp = extract_python_code(code_formatted)\n",
    "                \n",
    "#                 print(\"Code extracted after error\")\n",
    "#                 print(tmp)\n",
    "                execution_result = run_code_with_custom_globals(tmp[0])\n",
    "                try:\n",
    "                    execution_result = run_code_with_custom_globals(tmp[0])\n",
    "                    f_code.write(str(tmp[0])+\"\\n\")\n",
    "                    if('Error' in execution_result):\n",
    "                        print(\"error still exists\")\n",
    "                        print(execution_result)\n",
    "                        print(tmp[0])\n",
    "                        break\n",
    "                except:\n",
    "                    print(\"error in  tmp error\")\n",
    "                    print(tmp)\n",
    "                    print(regenerated_code)\n",
    "                    print(code_formatted)\n",
    "        \n",
    "\n",
    "            code_prompt+=step\n",
    "            code_prompt+=code_formatted\n",
    "            f.write(str(execution_result) + \"\\n\")\n",
    "            #print(execution_result)\n",
    "    f.close()\n",
    "    f_error.close()\n",
    "    f_code.close()\n",
    "    return code\n",
    "\n",
    "\n",
    "\n",
    "#    Generate Python code for the given request. Import the necessary libraries. Generate only the code so that I can directly execute. Do not use fbprophet only use prophet\n",
    "#     The database in discussion is df (pandas dataframe).Do not assume the existence of variables. Create them if required and use them. Use proper variable names as defined, do not assume variable names.\n",
    "#     Clearly define the target variable name based on previous steps.\n",
    "#    10. After identifying and extracting the necessary features for modeling, systematically remove rows containing None values to ensure dataset integrity\n",
    "\n",
    "\n",
    "\n",
    "plugin_schemas = {}\n",
    "generated_code = generate_run_code(plan_steps, plugin_schemas)\n",
    "# tmp = extract_python_code(generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6aaef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
