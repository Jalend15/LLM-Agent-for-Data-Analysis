import sqlite3
import pandas as pd

# Connect to the SQLite database
cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')

# Query the 'fires' table and select the necessary columns
query = "SELECT FIRE_YEAR, DISCOVERY_DOY, DISCOVERY_TIME, DISCOVERY_DATE, CONT_DOY, CONT_TIME, STAT_CAUSE_CODE, STAT_CAUSE_DESCR, FIRE_SIZE, LATITUDE, LONGITUDE, STATE, FIPS_NAME FROM fires"
df = pd.read_sql_query(query, cnx)

# Remove duplicate records
df.drop_duplicates(inplace=True)
# Date and Time Conversion
df["combined_date_dis"] = df["FIRE_YEAR"]*1000 + df["DISCOVERY_DOY"]
df["combined_date_dis"] = pd.to_datetime(df["combined_date_dis"], format="%Y%j")

df["combined_date_con"] = df["FIRE_YEAR"]*1000 + df["CONT_DOY"]
df["combined_date_con"] = pd.to_datetime(df["combined_date_con"], format="%Y%j")

# Calculate the duration of the fire
df["fire_duration"] = df["combined_date_con"] - df["combined_date_dis"]

# Extract additional temporal features
df["discovery_weekday"] = df["combined_date_dis"].dt.weekday
df["discovery_month"] = df["combined_date_dis"].dt.month
# Check Data Balance
cause_counts = df['STAT_CAUSE_DESCR'].value_counts()
print(cause_counts)

# Correlation Analysis
numerical_features = df[['FIRE_YEAR', 'DISCOVERY_DOY', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'discovery_weekday', 'discovery_month']]
correlation_matrix = numerical_features.corr()
print(correlation_matrix)
from sklearn.preprocessing import LabelEncoder

# Handle Categorical Data: Apply label encoding to 'STATE' and 'FIPS_NAME' variables
label_encoders = {}
for col in ['STATE', 'FIPS_NAME']:
    label_encoders[col] = LabelEncoder()
    df[col] = label_encoders[col].fit_transform(df[col])

# Feature Selection: Select significant features for the predictive model
selected_features = ['FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'discovery_weekday', 'discovery_month', 'STATE', 'FIPS_NAME']

# Handle Cause Label: Implement Label Encoder for 'STAT_CAUSE_DESCR'
cause_label_encoder = LabelEncoder()
df['CAUSE_LABEL'] = cause_label_encoder.fit_transform(df['STAT_CAUSE_DESCR'])
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Data Splitting: Divide the data into training and testing sets
X = df[selected_features]
y = df['CAUSE_LABEL']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle Missing Data: Use SimpleImputer to impute missing values with the mean for numerical features
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

# Choose Algorithm: Train XGBoost Classifier
xgb_model = XGBClassifier()
xgb_model.fit(X_train_imputed, y_train)

# Predict on the test set
y_pred = xgb_model.predict(X_test_imputed)

# Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Analyze Feature Importance
feature_importances = xgb_model.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
print("Feature Importance:")
print(feature_importance_df)

# Generate Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=cause_label_encoder.classes_))
# Input data for prediction
input_data = {
    'FIRE_SIZE': [500],  # Example fire size in acres
    'LATITUDE': [35.6895],  # Example latitude for location
    'LONGITUDE': [-105.9378],  # Example longitude for location
    'discovery_weekday': [3],  # Example discovery weekday (0-6, 0=Monday, 1=Tuesday, ..., 6=Sunday)
    'discovery_month': [7],  # Example discovery month (1-12)
    'STATE': [10],  # Example encoded state value
    'FIPS_NAME': [25]  # Example encoded county value
}

# Create a DataFrame from the input data
input_df = pd.DataFrame(input_data)

# Impute missing values for prediction
input_imputed = imputer.transform(input_df)

# Make predictions using the trained XGBoost model
predictions = xgb_model.predict(input_imputed)

# Decode the predicted cause labels
predicted_causes = cause_label_encoder.inverse_transform(predictions)

print("Predicted Cause of Wildfire:")
print(predicted_causes)
# Model Insights and Reporting
# Comprehensive Insights:
print("Comprehensive Insights:")
print("The trained XGBoost model can predict the cause of wildfires based on various input factors with a good level of accuracy. It provides a valuable tool for understanding and potentially preventing future incidents.")

# Highlight Predictive Factors:
print("\nPredictive Factors:")
print("1. Fire Size: Larger fire sizes may be more indicative of certain causes, such as arson or lightning strikes.")
print("2. Location (Latitude and Longitude): Geographic areas with specific coordinates may be prone to certain types of fire causes, such as human activity near urban centers.")
print("3. Discovery Weekday and Month: The timing of the fire discovery can give insights into potential causes related to seasonal factors or day-of-week patterns.")
print("4. State and County (FIPS Name): Different regions have varying environmental conditions and human activities, influencing the likelihood of specific fire causes.")
print("\nRecommendations:")
print("1. Conduct further research on the relationship between fire causes and environmental factors specific to different regions.")
print("2. Implement targeted prevention strategies based on predictive factors to reduce the occurrence of wildfires.")
