# Importing necessary libraries
import pandas as pd
import sqlite3

# Establishing a connection to the SQLite database
cnx = sqlite3.connect('FPA_FOD_20170508.sqlite')

# Querying the 'fires' table and selecting the required columns
query = "SELECT FIRE_YEAR, DISCOVERY_DOY, DISCOVERY_TIME, CONT_DOY, CONT_TIME, STAT_CAUSE_CODE, STAT_CAUSE_DESCR, FIRE_SIZE, LATITUDE, LONGITUDE, STATE, COUNTY FROM fires"
df = pd.read_sql_query(query, cnx)

# Checking for missing values in the selected columns and removing duplicates
df = df[['FIRE_YEAR', 'DISCOVERY_DOY', 'DISCOVERY_TIME', 'CONT_DOY', 'CONT_TIME', 'STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'STATE', 'COUNTY']]
df = df.dropna()
df = df.drop_duplicates()

# Displaying the cleaned dataset
print(df.head())
# Convert 'FIRE_YEAR' and 'DISCOVERY_DOY' to a datetime format to create 'combined_date_dis' column
df["combined_date_dis"] = df["FIRE_YEAR"]*1000 + df["DISCOVERY_DOY"]
df["combined_date_dis"] = pd.to_datetime(df["combined_date_dis"], format="%Y%j")

# Combine 'FIRE_YEAR' and 'CONT_DOY' to create 'combined_date_con' to include both discovery and containment times
df["combined_date_con"] = df["FIRE_YEAR"]*1000 + df["CONT_DOY"]
df["combined_date_con"] = pd.to_datetime(df["combined_date_con"], format="%Y%j")

# Calculate the duration of the fire by subtracting the discovery datetime from the containment datetime
df["fire_duration"] = df["combined_date_con"] - df["combined_date_dis"]

# Extract additional temporal features like weekday and month from the discovery date
df["weekday"] = df["combined_date_dis"].dt.weekday
df["month"] = df["combined_date_dis"].dt.month

# Displaying the updated dataset with new temporal features
print(df.head())
# Perform Exploratory Data Analysis (EDA)

# Checking the balance of categories within 'STAT_CAUSE_DESCR'
cause_counts = df['STAT_CAUSE_DESCR'].value_counts()
print("Frequency of different fire causes:")
print(cause_counts)

# Plan for data resampling if significant imbalances are detected

# Calculate correlation matrix for all numerical features
numerical_features = df[['FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'fire_duration', 'weekday', 'month']]
correlation_matrix = numerical_features.corr()

# Visualize the correlation matrix
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix of Numerical Features")
plt.show()

# Identify highly correlated features
# Consider dropping highly correlated or redundant features to simplify the model

# Example: Dropping highly correlated features
# For example, if 'FIRE_SIZE' and 'fire_duration' are highly correlated, you can drop one of them
# df = df.drop(['fire_duration'], axis=1)

# Display the updated dataset after dropping highly correlated features
# print(df.head())
# Feature Engineering Continued

# Apply label encoding to categorical variables like 'STATE' and 'COUNTY' due to their high cardinality
from sklearn.preprocessing import LabelEncoder

# Encode 'STATE' using LabelEncoder
state_encoder = LabelEncoder()
df['STATE'] = state_encoder.fit_transform(df['STATE'])

# Encode 'COUNTY' using LabelEncoder
county_encoder = LabelEncoder()
df['COUNTY'] = county_encoder.fit_transform(df['COUNTY'])

# Select significant features for the model, focusing on those most relevant for predicting fire causes
# Here, we will focus on 'STATE', 'COUNTY', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'weekday', and 'month'
selected_features = ['STATE', 'COUNTY', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'weekday', 'month']

# Implement Label Encoder for 'STAT_CAUSE_DESCR' instead of 'STAT_CAUSE_CODE' to ensure proper ordering of the labels
cause_encoder = LabelEncoder()
df['STAT_CAUSE_DESCR'] = cause_encoder.fit_transform(df['STAT_CAUSE_DESCR'])

# Display the updated dataset with encoded categorical variables and selected features
print(df[selected_features + ['STAT_CAUSE_DESCR']].head())
# Model Preparation

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

X = df[selected_features]
y = df['STAT_CAUSE_DESCR']

# Split the data into training and testing sets with an 80-20 split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Decide on strategies for handling missing data
# Depending on the amount and impact of missing data, consider imputing values or dropping rows for key features used in the models
# Here, we will provide examples of both strategies:

# Example 1: Imputing missing values with mean for numerical features
from sklearn.impute import SimpleImputer

# Impute missing values in numerical features with the mean
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Example 2: Dropping rows with missing values for categorical features
# Dropping rows with missing values for 'COUNTY' if it impacts the model significantly
X_train_dropna = X_train.dropna(subset=['COUNTY'])
X_test_dropna = X_test.dropna(subset=['COUNTY'])

# Display the imputed and/or dropped datasets for further model building
print("Sample of Imputed Data:")
print(pd.DataFrame(X_train_imputed, columns=selected_features).head())
print("\nSample of Data with Dropped Rows:")
print(X_train_dropna.head())
# Model Training and Evaluation

# Train a machine learning model such as XGBoost for handling the dataset's categorical nature and complexity
from xgboost import XGBClassifier
from sklearn.metrics import classification_report

# Initialize the XGBoost Classifier
xgb_model = XGBClassifier()

# Fit the model on the training data
xgb_model.fit(X_train_imputed, y_train)

# Predict on the test data
y_pred = xgb_model.predict(X_test_imputed)

# Generate a classification report to evaluate the model's performance
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Analyze feature importance to understand which factors most influence predicting fire causes
feature_importance = xgb_model.feature_importances_

# Create a DataFrame with feature importance scores
feature_importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the feature importance scores
print("\nFeature Importance Scores:")
print(feature_importance_df)
# Model Interpretation

# Provide comprehensive insights and recommendations based on the model outcomes

# Key Predictive Factors:
# - From the XGBoost model, the significant features influencing the prediction of fire causes are 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE', 'weekday', and 'month'.
# - 'FIRE_SIZE' appears to be the most important feature, indicating that the size of the fire has a substantial impact on the predicted cause.
# - Geospatial features like 'LATITUDE' and 'LONGITUDE' also play a crucial role, suggesting that the location of the fire is significant in determining its cause.
# - Temporal features such as 'weekday' and 'month' provide insights into the patterns of fire causes based on different days of the week and months of the year.

# Recommendations:
# - Focus on monitoring and managing fires based on their size, given its high importance in predicting causes. Implement strategies to control and prevent large fires.
# - Develop geospatial tools and technologies to identify hotspots and high-risk areas based on latitude and longitude information to proactively address fire causes.
# - Consider temporal variations in fire causes by adapting firefighting strategies to different weekdays and months, potentially optimizing resource allocation.
# - Enhance data collection and analysis to include additional contextual variables that may further improve predictive accuracy and identify nuanced patterns.

# Potential Areas for Further Research or Operational Integration:
# - Explore the interaction effects between features in the model to discover nonlinear relationships that may provide deeper insights into fire causes.
# - Investigate the impact of external factors such as weather conditions, land use, and human activities on fire causes to enhance predictive models.
# - Implement real-time data integration and machine learning to develop predictive tools for early fire cause detection and rapid response.
# - Collaborate with fire management agencies to integrate the model into operational workflows for optimized resource allocation and emergency response.
# - Conduct periodic model evaluations and updates to ensure relevance and effectiveness in addressing evolving fire cause dynamics.

# Print Final Answer
print("Comprehensive insights and recommendations based on the model outcomes have been provided.")
